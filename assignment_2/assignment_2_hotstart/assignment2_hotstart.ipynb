{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888dcbdc",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 2: Transformer Architecture Exercise\n",
    "\n",
    "This notebook serves as a reference implementation for **AssignmentÂ 2** of the generative AI course.  The goal is to compare three prominent transformer architecturesâ€”**decoderâ€‘only**, **encoderâ€‘only**, and **encoderâ€‘decoder**â€”on a common generative task.  The assignment requires training each architecture on the same dataset, evaluating their performance with common metrics, and analysing the implications of architectural differences on generative tasks and chainâ€‘ofâ€‘thought reasoning.\n",
    "\n",
    "## Dataset selection\n",
    "\n",
    "For this exercise we use the **CNN/DailyMail** summarisation dataset (versionÂ `3.0.0`) from HuggingÂ Faceâ€™s `datasets` library.  The dataset comprises news articles paired with humanâ€‘written summaries; each articleâ€“summary pair provides a natural input/output example for a generative model.  Because the data are already split into training/validation/test splits and are widely used for abstractive summarisation research, this dataset is appropriate for comparing generative architectures.  Although `WikiText` could be used for language modelling tasks, summarisation requires models to generate structured output given an input, which better illustrates differences between decoderâ€‘only, encoderâ€‘only, and encoderâ€‘decoder designs.  For compute efficiency in this notebook we subsample the dataset (e.g. a few hundred training examples) rather than using the full corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb4b15",
   "metadata": {},
   "source": [
    "\n",
    "## Overview of transformer architectures\n",
    "\n",
    "We train three different transformer models:\n",
    "\n",
    "* **Decoderâ€‘only (GPTâ€‘style):** These models consist of stacked selfâ€‘attention blocks in which each token can attend only to previous tokens (causal masking).  We use `GPTâ€‘2` as the base model and fineâ€‘tune it to generate a summary from an article.  Because GPTâ€‘2 is a pure language model, we construct input prompts of the form `\"summarize: <article>\"` and train the model to predict the target summary.  During training we mask out the prompt part of the input so that the loss is computed only on the summary tokens.\n",
    "\n",
    "* **Encoderâ€‘only (BERTâ€‘style):** Encoderâ€‘only models such as `BERT` learn biâ€‘directional contextual representations using masked language modelling (MLM).  They are not inherently generative; they excel at understanding tasks (e.g. classification, token classification).  For a fair comparison on generative tasks we fineâ€‘tune BERT on the same corpus using MLM, combining article and summary text into a single sequence.  At evaluation time we assess perplexity and use the `fillâ€‘mask` capability to approximate generation.  This highlights BERTâ€™s limitations on tasks requiring freeâ€‘form generation.\n",
    "\n",
    "* **Encoderâ€‘decoder (T5â€‘style):** Models like `T5` encode the input sequence with an encoder and decode the output sequence with a separate decoder.  They can perform a wide range of textâ€‘toâ€‘text tasks, including summarisation and question answering.  We fineâ€‘tune `T5â€‘small` on the CNN/DailyMail dataset using the standard prefix `\"summarize: \"` in the input to indicate the task.  During evaluation we compute ROUGE metrics on generated summaries.\n",
    "\n",
    "The following sections implement data loading, preprocessing, model fineâ€‘tuning, and evaluation for each architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d888988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "from transformers import logging\n",
    "\n",
    "# Silence warnings for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542eddf5",
   "metadata": {},
   "source": [
    "\n",
    "## Load and inspect the dataset\n",
    "\n",
    "We load the CNN/DailyMail dataset using the HuggingÂ Face `datasets` library.  To accelerate training for demonstration purposes we take a small subset of the training and validation sets (e.g. 500 training examples and 100 validation examples).  Each record contains two fields:\n",
    "\n",
    "* `\"article\"`: the news article text (input).\n",
    "* `\"highlights\"`: the humanâ€‘written summary (target).\n",
    "\n",
    "Below we load the dataset, inspect a few examples, and create the smaller subsets used for fineâ€‘tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7d3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': \"By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide poisoning from a cooker. Tragic: The inquests have opened into the deaths of three members of the same family who were found in their static caravan last weekend. John and Audrey Cook are pictured . Awful: The family died following carbon monoxide poisoning at this caravan at the Tremarle Home Park in Camborne, Cornwall . It is also believed there was no working carbon monoxide detector in the static caravan. Cornwall Fire and Rescue Service said this would have resulted in the three being unconscious 'within minutes', . A spokesman for Cornwall coroner Dr Emma Carlyon confirmed the inquests were opened and adjourned yesterday afternoon. They will resume at a later date. Devon and Cornwall Police confirmed on Monday that carbon monoxide poisoning had been established as the cause of death. A police spokesman said the source of the poisoning was 'believed to be from incorrect operation of the gas cooker'. Poisoning: This woman left flowers outside the caravan following the deaths. It has emerged that the trio would have been unconscious 'within minutes' Touching: This tribute was left outside the caravan following news of the deaths . Early readings from experts at the site revealed a potentially lethal level of carbon monoxide present within the caravan at the time it was taken, shortly after the discovery of the bodies. Friends and neighbours have paid tribute to the trio. One . neighbour, Sonya Owen, 53, said: 'It's very distressing. I knew the . daughter, she was living her with her mum and dad. Everybody is really . upset.' Margaret Holmes, 65, who lived near the couple and their . daughter, said: 'They had lived here for around 40 years and they kept . themselves to themselves. 'I just canâ€™t believe this has . happened, it is so sad and I am so shocked, I think we all are, you just . donâ€™t expect this sort of thing to happen on your doorstep. 'Everyone will miss them, we used to chat a lot when we were both in the garden. 'I would just like to send my condolences to their family, I canâ€™t imagine what theyâ€™re going through.' Nic Clark, 52, who was good friends with daughter Maureen, added: 'They were a lovely kind family, a great trio. 'Maureen . used to go out and walk her dog, a little Jack Russell, it is so sad . what has happened, I understand the dog went with them. 'They . will be sorely missed and I think everyone is just in shock at the . moment, I would like to send my condolences to the Cook family.'\", 'highlights': 'John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'id': '08cf276c9eadb638e0c7fdc83ce0229c8af5d09b'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
    "\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9c398",
   "metadata": {},
   "source": [
    "\n",
    "## Decoderâ€‘only model: GPTâ€‘2 fineâ€‘tuning\n",
    "\n",
    "A decoderâ€‘only transformer must learn to generate a summary given an input article.  We use a promptâ€‘based approach: the input text has the form `\"summarize: <article>\"`, and the model is trained to produce the summary tokens.  To prevent the model from learning to predict the prompt tokens, we mask the loss on the prompt portion of the sequence (by setting corresponding labels to `-100`).\n",
    "\n",
    "We use the `GPTâ€‘2` tokenizer and model from HuggingÂ Face.  Because GPTâ€‘2 lacks a padding token by default, we add a pad token equal to the endâ€‘ofâ€‘text token.  We then tokenize the inputs and construct labels accordingly.  The function below performs these steps and is mapped over the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6c9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Cam\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0862232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs and Labels dimensions for first 10 samples:\n",
      "Sample 0: Input IDs length = 512, Labels length = 512\n",
      "Sample 1: Input IDs length = 512, Labels length = 512\n",
      "Sample 2: Input IDs length = 512, Labels length = 512\n",
      "Sample 3: Input IDs length = 512, Labels length = 512\n",
      "Sample 4: Input IDs length = 512, Labels length = 512\n",
      "Sample 5: Input IDs length = 512, Labels length = 512\n",
      "Sample 6: Input IDs length = 512, Labels length = 512\n",
      "Sample 7: Input IDs length = 512, Labels length = 512\n",
      "Sample 8: Input IDs length = 512, Labels length = 512\n",
      "Sample 9: Input IDs length = 512, Labels length = 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs and Labels dimensions for first 10 samples:\")\n",
    "for i in range(10):\n",
    "    print(f\"Sample {i}: Input IDs length = {len(train_gpt2[i]['input_ids'])}, Labels length = {len(train_gpt2[i]['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67ece",
   "metadata": {},
   "source": [
    "\n",
    "### GPTâ€‘2 training configuration\n",
    "\n",
    "We use the HuggingÂ Face `Trainer` API to fineâ€‘tune the GPTâ€‘2 model.  A `DataCollatorForLanguageModeling` automatically pads the inputs and labels and performs dynamic masking where appropriate (although in our custom loss masking we already set `-100` values).  The training arguments below specify a small number of epochs and batch sizes for illustration; adjust these for a full training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1ccb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate: 1.10.1\n",
      "torch: 2.8.0\n",
      "transformers: 4.56.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate, torch, transformers\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"transformers:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a0222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1534, 'grad_norm': 3.9447619915008545, 'learning_rate': 3.237410071942446e-05, 'epoch': 1.592}\n",
      "{'eval_loss': 2.940354347229004, 'eval_runtime': 6.0791, 'eval_samples_per_second': 16.45, 'eval_steps_per_second': 8.225, 'epoch': 1.592}\n",
      "{'train_runtime': 328.1391, 'train_samples_per_second': 4.571, 'train_steps_per_second': 0.576, 'train_loss': 3.0467438874421298, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=3.0467438874421298, metrics={'train_runtime': 328.1391, 'train_samples_per_second': 4.571, 'train_steps_per_second': 0.576, 'train_loss': 3.0467438874421298, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define data collator\n",
    "data_collator_gpt2 = DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False)\n",
    "\n",
    "# Load the GPT-2 model\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Training arguments\n",
    "training_args_gpt2 = TrainingArguments(\n",
    "    output_dir=\"./gpt2-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],  # disable logging to wandb\n",
    ")\n",
    "\n",
    "# Create Trainer for GPT-2\n",
    "trainer_gpt2 = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args_gpt2,\n",
    "    train_dataset=train_gpt2,\n",
    "    eval_dataset=val_gpt2,\n",
    "    data_collator=data_collator_gpt2,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train; training can take several minutes even on small subsets\n",
    "trainer_gpt2.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "443470a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "summarize: The quick brown fox jumps over the lazy dog. The dog is so happy to be home, but the fox is so scared of the dog that he jumps over the lazy dog. The dog is so happy to be home, but\n"
     ]
    }
   ],
   "source": [
    "# Generated by GPT-5 (high reasoning)\n",
    "# Prompt: Can you help me with this error? (ERROR with mps/apple silicon acceleration)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Choose device\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Ensure GPT-2 has a pad token and move model to device in float32\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_model.config.pad_token_id = gpt2_tokenizer.pad_token_id\n",
    "\n",
    "gpt2_model.to(device=device, dtype=torch.float32)\n",
    "gpt2_model.eval()\n",
    "\n",
    "sample_input = \"summarize: The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = gpt2_tokenizer(sample_input, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Warm-up pass to force allocation on MPS\n",
    "with torch.no_grad():\n",
    "    _ = gpt2_model(input_ids=inputs[\"input_ids\"][:, :1], attention_mask=torch.ones_like(inputs[\"input_ids\"][:, :1]))\n",
    "\n",
    "# Generate (pass the whole dict so attention_mask is used)\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2_model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False,  # greedy; change if you want sampling\n",
    "    )\n",
    "\n",
    "print(gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70f5d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary:  The dog is so happy to be home, but the fox is so scared of the dog that he jumps over the lazy dog. The dog is so happy to be home, but\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample input\n",
    "sample_input = \"summarize: The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = gpt2_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "outputs = gpt2_model.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
    "text_out= gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated summary:\", text_out[len(sample_input):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16871507",
   "metadata": {},
   "source": [
    "## What do you observe in the output?\n",
    "1. Can you postprocess the output so that it only starts printing after the input sequence?\n",
    "2. Can you iterate and improve the summary quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29cc30",
   "metadata": {},
   "source": [
    "\n",
    "## Encoderâ€‘only model: BERT fineâ€‘tuning\n",
    "\n",
    "BERT uses biâ€‘directional selfâ€‘attention and is optimised for understanding rather than generation.  To apply BERT on our corpus we fineâ€‘tune it using the **masked language modelling (MLM)** objective.  We concatenate the article and its summary into a single sequence and randomly mask tokens using `DataCollatorForLanguageModeling`.  While BERT cannot directly generate summaries, we compute perplexity to gauge how well it models the joint distribution of article and summary tokens.  At evaluation we also demonstrate how to use the `fill-mask` pipeline to generate single masked words as an illustration of BERTâ€™s generative limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bee9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 samples of the dataset:\n",
      "small_train_dataset[0]: dict_keys(['article', 'highlights', 'id'])\n",
      "\t\tsmall_train_dataset[0]: By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a sta... \n",
      "\t->\n",
      "\t\tJohn and .\n",
      "Audrey Cook were discovered alongside their daughter, Maureen .\n",
      "They were found at Tremarle Home Park in Cornwall .\n",
      "Investigators say the three died of carbon monoxide .\n",
      "poisoning .\n",
      "\t\tsmall_train_dataset[1]: UNITED NATIONS (CNN) -- A rare meeting of U.N. Security Council heads of state, led for the first time by a U.S. president, adopted a resolution focus... \n",
      "\t->\n",
      "\t\tNEW: Libya can serve as example of cooperation, White House spokesman says .\n",
      "Resolution calls for preventing nuclear weapons from being stolen, used by military .\n",
      "Obama, Russian President Dimitry Medvedev working to reduce stockpiles .\n",
      "Venezuelan president Hugo Chavez on \"Larry King Live\" tonight, 9 ET .\n",
      "\t\tsmall_train_dataset[2]: Cover-up: Former Archbishop Lord Hope allowed a paedophile priest to escape punishment for sex crimes, a judge's report claims . A former archbishop w... \n",
      "\t->\n",
      "\t\tVery Reverend Robert Waddington sexually abused choirboys for decades .\n",
      "Inquiry into the abuse has slammedÂ Lord Hope, former Archbishop of York .\n",
      "Report claims he was made aware of misconduct 19 times but did not act .\n",
      "Despite this - he still holds influential post and sits in the House of Lords .\n",
      "\t\tsmall_train_dataset[3]: By . Kristie Lau . PUBLISHED: . 10:48 EST, 14 June 2012 . | . UPDATED: . 11:02 EST, 14 June 2012 . TLC has pulled an episode of Cake Boss from future ... \n",
      "\t->\n",
      "\t\tMonday night's episode showed Buddy Valastro tricking Anthony Bellifemine into thinking that Carmen Carerra, 27, was born as a woman .\n",
      "TLC removed the episode from future screening schedules .\n",
      "\t\tsmall_train_dataset[4]: 'The lamps are going out all over Europe. We shall not see them lit again in our lifetime' Foreign Secretary Sir Edward Grey, August 3, 1914 . Prime M... \n",
      "\t->\n",
      "\t\tPeople asked to turn out lights for hour between 10 and 11pm tomorrow .\n",
      "Gesture is inÂ remembrance of those killed in the First World War .\n",
      "Tower Bridge and 10 Downing Street will also extinguish all but one light .\n",
      "\t\tsmall_train_dataset[5]: Roy Hodgson has come under fire for making public Raheem Sterling's admission that he was feeling tired ahead of England's Euro 2016 qualifier with Es... \n",
      "\t->\n",
      "\t\tRoy Hodgson revealed that Raheem Sterling had told him he was feeling fatigued prior to England's Euro 2016 qualifier with Estonia .\n",
      "Sterling was dropped from the starting line-up in place of Adam Lallana .\n",
      "The Liverpool man later came on as a substitute as England won 1-0 .\n",
      "Hodgson was criticised by Sky Sports pundits Jamie Redknapp and Jamie Carragher for making Sterling's admission of tiredness public .\n",
      "Wayne Rooney's free-kick preserved England's perfect start to qualifying .\n",
      "\t\tsmall_train_dataset[6]: Every frontline police officer should be offered a Taser to help fight the threat from lone-wolf terrorists, a police leader declared yesterday. Steve... \n",
      "\t->\n",
      "\t\tCalls for every police officer to be offered a Taser to fight terrorist threat .\n",
      "Police Federation set to vote on giving all frontline officers training .\n",
      "Terrorist threat level for police was raised to severe after Paris attacks .\n",
      "\t\tsmall_train_dataset[7]: By . Dan Bloom . It's one very, very small step for man - but a giant leap for antkind. Around 800 common ants, usually found pattering across pavemen... \n",
      "\t->\n",
      "\t\t800 common pavement ants are now living on International Space Station .\n",
      "Scientists will examine how they work together in low gravity to find food .\n",
      "The ants' methods can then be copied to develop 'intelligent' search robots .\n",
      "\t\tsmall_train_dataset[8]: By . Jill Reilly . PUBLISHED: . 09:26 EST, 14 June 2012 . | . UPDATED: . 13:19 EST, 14 June 2012 . A pensioner died from fatal injuries after being hi... \n",
      "\t->\n",
      "\t\tDavid Wilcockson, 71, was bowling at a ground in  Cranleigh, Surrey when the ball struck him on the head .\n",
      "Died in hospital on June 1 after 13 days in a coma .\n",
      "He was the longest-serving member of the Old Dorkinians, joining the club in 1959 .\n",
      "The pensioner had set himself a target of 3,000 wickets - and died just 101 short .\n",
      "\t\tsmall_train_dataset[9]: Workers digging an underground garage for a new hotel  recently struck something big about 30 feet below the surface. This week they uncovered it - a ... \n",
      "\t->\n",
      "\t\tThe boulder was found by a construction crew in Everett, Washington .\n",
      "The rock is bigger than an SUV and close to 19 feet long .\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 samples of the dataset\n",
    "print(\"First 10 samples of the dataset:\")\n",
    "print(\"small_train_dataset[0]:\", small_train_dataset[0].keys())\n",
    "for i in range(10):\n",
    "    print(f\"\\t\\tsmall_train_dataset[{i}]: {small_train_dataset[i]['article'][:150]}... \\n\\t->\\n\\t\\t{small_train_dataset[i]['highlights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba715f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68bd134d91e4ea6b76f8d8e4079d84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2608, 'grad_norm': 9.490689277648926, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 1.8042117357254028, 'eval_runtime': 4.5599, 'eval_samples_per_second': 21.93, 'eval_steps_per_second': 5.483, 'epoch': 0.8}\n",
      "{'train_runtime': 92.1201, 'train_samples_per_second': 5.428, 'train_steps_per_second': 1.357, 'train_loss': 2.2258151245117186, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-mlm/tokenizer_config.json',\n",
       " './bert-mlm/special_tokens_map.json',\n",
       " './bert-mlm/vocab.txt',\n",
       " './bert-mlm/added_tokens.json',\n",
       " './bert-mlm/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model for BERT\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define preprocessing: combine article and summary\n",
    "\n",
    "def preprocess_bert(examples):\n",
    "    concatenated_texts = [\n",
    "        art + \" \" + summ \n",
    "        for art, summ in zip(examples[\"article\"], examples[\"highlights\"])\n",
    "    ]\n",
    "    model_inputs = bert_tokenizer(concatenated_texts, max_length=512, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_bert = small_train_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_bert = small_val_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Data collator with MLM\n",
    "mlm_probability = 0.15\n",
    "data_collator_bert = DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=True, mlm_probability=mlm_probability)\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "# Training arguments for BERT\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir=\"./bert-mlm\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_bert,\n",
    "    eval_dataset=val_bert,\n",
    "    data_collator=data_collator_bert,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train the BERT model\n",
    "trainer_bert.train()\n",
    "# save the model\n",
    "bert_model.save_pretrained(\"./bert-mlm\")\n",
    "bert_tokenizer.save_pretrained(\"./bert-mlm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49de6626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: torch.Size([1, 12, 30522]) logits\n",
      "News dataset loaded with 1000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5702, 'grad_norm': 0.7809127569198608, 'learning_rate': 4.2461538461538465e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.32723987102508545, 'eval_runtime': 42.2125, 'eval_samples_per_second': 23.69, 'eval_steps_per_second': 2.961, 'epoch': 0.8}\n",
      "{'loss': 0.3016, 'grad_norm': 0.46356937289237976, 'learning_rate': 2.7076923076923078e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 0.22899016737937927, 'eval_runtime': 41.9356, 'eval_samples_per_second': 23.846, 'eval_steps_per_second': 2.981, 'epoch': 1.6}\n",
      "{'loss': 0.2509, 'grad_norm': 0.398881196975708, 'learning_rate': 1.1692307692307693e-05, 'epoch': 2.4}\n",
      "{'eval_loss': 0.1859639286994934, 'eval_runtime': 41.5552, 'eval_samples_per_second': 24.064, 'eval_steps_per_second': 3.008, 'epoch': 2.4}\n",
      "{'train_runtime': 397.3076, 'train_samples_per_second': 7.551, 'train_steps_per_second': 0.944, 'train_loss': 0.3441635030110677, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.3441635030110677, metrics={'train_runtime': 397.3076, 'train_samples_per_second': 7.551, 'train_steps_per_second': 0.944, 'train_loss': 0.3441635030110677, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of bert model for sample input\n",
    "sample_input = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = bert_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "outputs = bert_model(**inputs)\n",
    "print(f\"Sample {i}: {outputs.logits.shape} logits\")       \n",
    "\n",
    "# Freeze the BERT model and train a classifier on top\n",
    "# load a news sentiment classification dataset\n",
    "from datasets import load_dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "# Use the trained model as a feature extractor and add a classification head\n",
    "def get_news_classifier_model():\n",
    "    \"\"\"\n",
    "    load the trained BERT model and add a classification head\n",
    "    \"\"\"\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"./bert-mlm\") # Load the trained BERT model\n",
    "    # Freeze the BERT model\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Add a classification head\n",
    "    model.classifier = torch.nn.Linear(model.config.hidden_size, 4)  # 4 classes for AG News\n",
    "    return model\n",
    "\n",
    "news_classifier_model = get_news_classifier_model()\n",
    "# Training arguments for news classifier\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=False),\n",
    ")\n",
    "# Uncomment the line below to train the news classifier\n",
    "news_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3e0b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/transformers/training_args.py:2278: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News dataset loaded with 1000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yr/p03j319114v61sg1cs61p6r80000gn/T/ipykernel_5736/2083709714.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  news_trainer = Trainer(\n",
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0303, 'grad_norm': 4.586305141448975, 'learning_rate': 4.831034482758621e-05, 'epoch': 0.2}\n",
      "{'eval_loss': 0.535821795463562, 'eval_runtime': 34.7279, 'eval_samples_per_second': 28.795, 'eval_steps_per_second': 14.398, 'epoch': 0.2}\n",
      "{'loss': 0.9805, 'grad_norm': 0.09522397071123123, 'learning_rate': 4.486206896551725e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 1.0917686223983765, 'eval_runtime': 34.1975, 'eval_samples_per_second': 29.242, 'eval_steps_per_second': 14.621, 'epoch': 0.4}\n",
      "{'loss': 0.8689, 'grad_norm': 0.26558780670166016, 'learning_rate': 4.141379310344828e-05, 'epoch': 0.6}\n",
      "{'eval_loss': 0.5263210535049438, 'eval_runtime': 34.6338, 'eval_samples_per_second': 28.874, 'eval_steps_per_second': 14.437, 'epoch': 0.6}\n",
      "{'loss': 0.6004, 'grad_norm': 0.05033459886908531, 'learning_rate': 3.796551724137931e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.5818000435829163, 'eval_runtime': 33.9352, 'eval_samples_per_second': 29.468, 'eval_steps_per_second': 14.734, 'epoch': 0.8}\n",
      "{'loss': 0.7289, 'grad_norm': 0.16762292385101318, 'learning_rate': 3.451724137931035e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5890069603919983, 'eval_runtime': 33.8982, 'eval_samples_per_second': 29.5, 'eval_steps_per_second': 14.75, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3806, 'grad_norm': 15.623650550842285, 'learning_rate': 3.106896551724138e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 0.4662708044052124, 'eval_runtime': 33.9337, 'eval_samples_per_second': 29.469, 'eval_steps_per_second': 14.735, 'epoch': 1.2}\n",
      "{'loss': 0.6352, 'grad_norm': 20.103633880615234, 'learning_rate': 2.7620689655172417e-05, 'epoch': 1.4}\n",
      "{'eval_loss': 0.3645251393318176, 'eval_runtime': 33.9655, 'eval_samples_per_second': 29.442, 'eval_steps_per_second': 14.721, 'epoch': 1.4}\n",
      "{'loss': 0.4072, 'grad_norm': 0.36894330382347107, 'learning_rate': 2.417241379310345e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 0.2987467050552368, 'eval_runtime': 33.917, 'eval_samples_per_second': 29.484, 'eval_steps_per_second': 14.742, 'epoch': 1.6}\n",
      "{'loss': 0.3727, 'grad_norm': 0.050471995025873184, 'learning_rate': 2.0724137931034484e-05, 'epoch': 1.8}\n",
      "{'eval_loss': 0.272615522146225, 'eval_runtime': 33.9307, 'eval_samples_per_second': 29.472, 'eval_steps_per_second': 14.736, 'epoch': 1.8}\n",
      "{'loss': 0.5006, 'grad_norm': 0.19159317016601562, 'learning_rate': 1.727586206896552e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.2411702573299408, 'eval_runtime': 34.0072, 'eval_samples_per_second': 29.406, 'eval_steps_per_second': 14.703, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2503, 'grad_norm': 0.032772358506917953, 'learning_rate': 1.3827586206896554e-05, 'epoch': 2.2}\n",
      "{'eval_loss': 0.2828271985054016, 'eval_runtime': 34.7492, 'eval_samples_per_second': 28.778, 'eval_steps_per_second': 14.389, 'epoch': 2.2}\n",
      "{'loss': 0.3287, 'grad_norm': 58.31756591796875, 'learning_rate': 1.0379310344827587e-05, 'epoch': 2.4}\n",
      "{'eval_loss': 0.18533147871494293, 'eval_runtime': 35.7811, 'eval_samples_per_second': 27.948, 'eval_steps_per_second': 13.974, 'epoch': 2.4}\n",
      "{'loss': 0.3249, 'grad_norm': 0.044865768402814865, 'learning_rate': 6.931034482758621e-06, 'epoch': 2.6}\n",
      "{'eval_loss': 0.15776297450065613, 'eval_runtime': 35.7905, 'eval_samples_per_second': 27.94, 'eval_steps_per_second': 13.97, 'epoch': 2.6}\n",
      "{'loss': 0.2409, 'grad_norm': 14.798412322998047, 'learning_rate': 3.4827586206896552e-06, 'epoch': 2.8}\n",
      "{'eval_loss': 0.15635497868061066, 'eval_runtime': 35.871, 'eval_samples_per_second': 27.878, 'eval_steps_per_second': 13.939, 'epoch': 2.8}\n",
      "{'loss': 0.1036, 'grad_norm': 0.07337309420108795, 'learning_rate': 3.448275862068965e-08, 'epoch': 3.0}\n",
      "{'eval_loss': 0.15144692361354828, 'eval_runtime': 35.7028, 'eval_samples_per_second': 28.009, 'eval_steps_per_second': 14.004, 'epoch': 3.0}\n",
      "{'train_runtime': 1000.1527, 'train_samples_per_second': 3.0, 'train_steps_per_second': 1.5, 'train_loss': 0.5169038111368816, 'epoch': 3.0}\n",
      "Model actual device: mps:0\n",
      "Sample news input: Breaking news: The stock market crashes as investors panic.\n",
      "Predicted class: 2\n",
      "Available classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Code revised by GPT-5 (high reasoning) via Windsurf - fixing error to do with mac gpu acceleration\n",
    "\n",
    "# Print sample output of the news classifier\n",
    "# The previous approach used AutoModelForMaskedLM, which is not suitable for classification.\n",
    "# Instead, use AutoModelForSequenceClassification for news classification.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Choose device (prefer MPS on macOS if available)\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load a BERT model with a classification head and move to device in float32\n",
    "news_classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    bert_model_name, num_labels=4\n",
    ").to(device=device, dtype=torch.float32)\n",
    "\n",
    "# Training arguments for news classifier\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),  # keep fp16 off for MPS/CPU\n",
    "    report_to=[],\n",
    "    use_mps_device=torch.backends.mps.is_available(),  # ensure Trainer uses MPS if present\n",
    ")\n",
    "\n",
    "# Load the AG News dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(\n",
    "        examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    tokenizer=bert_tokenizer,\n",
    ")\n",
    "\n",
    "# Train (you can comment this out to just test inference)\n",
    "news_trainer.train()\n",
    "\n",
    "# -------- Inference (post-training) --------\n",
    "news_classifier_model.eval()\n",
    "\n",
    "# Get the actual device the Trainer placed the model on (MPS/CPU/CUDA)\n",
    "actual_device = next(news_classifier_model.parameters()).device\n",
    "print(\"Model actual device:\", actual_device)\n",
    "\n",
    "# Tokenize and move inputs to the model's device\n",
    "sample_news_input = \"Breaking news: The stock market crashes as investors panic.\"\n",
    "inputs = bert_tokenizer(\n",
    "    sample_news_input, return_tensors=\"pt\", truncation=True, padding=True\n",
    ")\n",
    "inputs = {k: v.to(actual_device) for k, v in inputs.items()}\n",
    "\n",
    "# Warm-up pass to materialize weights on MPS\n",
    "with torch.no_grad():\n",
    "    _ = news_classifier_model(\n",
    "        input_ids=inputs[\"input_ids\"][:, :1],\n",
    "        attention_mask=inputs[\"attention_mask\"][:, :1],\n",
    "    )\n",
    "\n",
    "# Actual forward\n",
    "with torch.no_grad():\n",
    "    outputs = news_classifier_model(**inputs)\n",
    "\n",
    "pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "print(f\"Sample news input: {sample_news_input}\")\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(\"Available classes:\", news_dataset.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c0127fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[207   1   3   1]\n",
      " [  0 142   0   0]\n",
      " [  1   0 160  13]\n",
      " [  1   0   2 469]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code revised by GPT-5 (high reasoning) via Windsurf - fixing error to do with mac gpu acceleration\n",
    "\n",
    " \n",
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Ensure eval and align devices\n",
    "news_classifier_model.eval()\n",
    "actual_device = next(news_classifier_model.parameters()).device\n",
    "\n",
    "predictions = []\n",
    "for i in range(0, len(train_news), 32):\n",
    "    batch = train_news[i:i+32]  # dict of lists\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(actual_device),\n",
    "        \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(actual_device),\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = news_classifier_model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    predictions.extend(preds)\n",
    "\n",
    "# True labels\n",
    "true_labels = np.array(train_news[\"labels\"])\n",
    "\n",
    "#\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=news_dataset.features[\"label\"].names,\n",
    "            yticklabels=news_dataset.features[\"label\"].names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for News Classifier\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ee761",
   "metadata": {},
   "source": [
    "\n",
    "## Encoderâ€‘decoder model: T5 fineâ€‘tuning\n",
    "\n",
    "T5 is a textâ€‘toâ€‘text model that uses a separate encoder and decoder.  It naturally handles generative tasks such as summarisation.  We prepend the prefix `\"summarize: \"` to each article, then tokenize the input and the summary separately.  A `DataCollatorForSeq2Seq` takes care of padding the inputs and shifting the decoder labels.  During evaluation we use greedy decoding to produce summaries and compute ROUGE scores against the reference summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14ed723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': \"By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide poisoning from a cooker. Tragic: The inquests have opened into the deaths of three members of the same family who were found in their static caravan last weekend. John and Audrey Cook are pictured . Awful: The family died following carbon monoxide poisoning at this caravan at the Tremarle Home Park in Camborne, Cornwall . It is also believed there was no working carbon monoxide detector in the static caravan. Cornwall Fire and Rescue Service said this would have resulted in the three being unconscious 'within minutes', . A spokesman for Cornwall coroner Dr Emma Carlyon confirmed the inquests were opened and adjourned yesterday afternoon. They will resume at a later date. Devon and Cornwall Police confirmed on Monday that carbon monoxide poisoning had been established as the cause of death. A police spokesman said the source of the poisoning was 'believed to be from incorrect operation of the gas cooker'. Poisoning: This woman left flowers outside the caravan following the deaths. It has emerged that the trio would have been unconscious 'within minutes' Touching: This tribute was left outside the caravan following news of the deaths . Early readings from experts at the site revealed a potentially lethal level of carbon monoxide present within the caravan at the time it was taken, shortly after the discovery of the bodies. Friends and neighbours have paid tribute to the trio. One . neighbour, Sonya Owen, 53, said: 'It's very distressing. I knew the . daughter, she was living her with her mum and dad. Everybody is really . upset.' Margaret Holmes, 65, who lived near the couple and their . daughter, said: 'They had lived here for around 40 years and they kept . themselves to themselves. 'I just canâ€™t believe this has . happened, it is so sad and I am so shocked, I think we all are, you just . donâ€™t expect this sort of thing to happen on your doorstep. 'Everyone will miss them, we used to chat a lot when we were both in the garden. 'I would just like to send my condolences to their family, I canâ€™t imagine what theyâ€™re going through.' Nic Clark, 52, who was good friends with daughter Maureen, added: 'They were a lovely kind family, a great trio. 'Maureen . used to go out and walk her dog, a little Jack Russell, it is so sad . what has happened, I understand the dog went with them. 'They . will be sorely missed and I think everyone is just in shock at the . moment, I would like to send my condolences to the Cook family.'\", 'highlights': 'John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'id': '08cf276c9eadb638e0c7fdc83ce0229c8af5d09b'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae64141460747908435e1fcdacb28c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ee3969ade84165ab382aee8de0de7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3d8dc7a8104ba6901181b5fe9f1e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46202532bb624327881847bed8eec990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004d570e2d2f4a2aa522903f9a42c35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854874b4f6e040fe91a8370be42b90d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd0393b530742e3ab5bf27f01902e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32c1a2122c84beb9c8729e21e677521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yr/p03j319114v61sg1cs61p6r80000gn/T/ipykernel_5031/3507327657.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_t5 = Trainer(\n",
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 47.8178, 'train_samples_per_second': 10.456, 'train_steps_per_second': 1.318, 'train_loss': 2.4085518973214284, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=2.4085518973214284, metrics={'train_runtime': 47.8178, 'train_samples_per_second': 10.456, 'train_steps_per_second': 1.318, 'train_loss': 2.4085518973214284, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "def preprocess_t5(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = t5_tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = t5_tokenizer(examples[\"highlights\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_t5 = small_train_dataset.map(preprocess_t5, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_t5 = small_val_dataset.map(preprocess_t5, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Data collator for seq2seq tasks\n",
    "data_collator_t5 = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model_name)\n",
    "\n",
    "# Load T5 model\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_name)\n",
    "\n",
    "# Training arguments for T5\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir=\"./t5-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=train_t5,\n",
    "    eval_dataset=val_t5,\n",
    "    data_collator=data_collator_t5,\n",
    "    tokenizer=t5_tokenizer,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train the T5 model\n",
    "trainer_t5.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1ffa7",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation\n",
    "\n",
    "After fineâ€‘tuning the models (training steps are commented out by default), we evaluate them on the validation subset. Different metrics are appropriate for each architecture:\n",
    "\n",
    "* **GPTâ€‘2** (decoderâ€‘only): We generate summaries using greedy decoding and compute ROUGE metrics (ROUGEâ€‘1, ROUGEâ€‘2, ROUGEâ€‘L). We also compute perplexity using the loss returned by the trainer.\n",
    "\n",
    "* **BERT** (encoderâ€‘only): BERT is not designed to generate full sequences; instead we use it for downstream tasks such as text classification. For a classification scenario, the evaluation metrics are typically confusion matrix and F1-score.\n",
    "\n",
    "* **T5** (encoderâ€‘decoder): We generate summaries using greedy decoding and compute ROUGE metrics.  Perplexity is computed similarly to GPTâ€‘2 by exponentiating the validation loss.\n",
    "\n",
    "The code below demonstrates evaluation routines for each model. Running these functions requires trained models; if you skipped training above, the evaluation will use the preâ€‘trained weights and therefore will not yield good summarization quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d4253cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkutyna/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.945127010345459, 'eval_runtime': 10.8706, 'eval_samples_per_second': 9.199, 'eval_steps_per_second': 4.6, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 394, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m gpt2_eval_results = trainer_gpt2.evaluate()\n\u001b[32m     31\u001b[39m gpt2_perplexity = compute_perplexity(gpt2_eval_results)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m rouge_gpt2 = \u001b[43mevaluate_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt2_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmall_val_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPT-2 Perplexity:\u001b[39m\u001b[33m\"\u001b[39m, gpt2_perplexity)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPT-2 ROUGE:\u001b[39m\u001b[33m\"\u001b[39m, rouge_gpt2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mevaluate_gpt2\u001b[39m\u001b[34m(model, tokenizer, dataset, num_samples)\u001b[39m\n\u001b[32m     15\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m512\u001b[39m).to(model.device)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m summary = tokenizer.decode(output_ids[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m preds.append(summary)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2386\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supports_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlogits_to_keep\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2384\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mlogits_to_keep\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2386\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2388\u001b[39m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[32m   2389\u001b[39m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[32m   2390\u001b[39m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[32m   2391\u001b[39m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[32m   2392\u001b[39m max_cache_length = generation_config.max_length - \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CascadeProjects/MSIT3103/MSIT3103-GenAI-assignments/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:1602\u001b[39m, in \u001b[36mGenerationMixin._validate_generated_length\u001b[39m\u001b[34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[39m\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length >= generation_config.max_length:\n\u001b[32m   1601\u001b[39m     input_ids_string = \u001b[33m\"\u001b[39m\u001b[33mdecoder_input_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1602\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1603\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but `max_length` is set to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1604\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config.max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1605\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1606\u001b[39m     )\n\u001b[32m   1608\u001b[39m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[32m   1609\u001b[39m min_length_error_suffix = (\n\u001b[32m   1610\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mincrease the maximum length.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1612\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Input length of input_ids is 394, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "\n",
    "# Define ROUGE metric\n",
    "evaluate_rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics_rouge(preds, refs):\n",
    "    # Compute ROUGE scores; use newline separation between sentences in each text\n",
    "    result = evaluate_rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "# Function to generate summaries with GPT-2\n",
    "def evaluate_gpt2(model, tokenizer, dataset, num_samples=10):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for i, example in enumerate(dataset.select(range(num_samples))):\n",
    "        prompt = \"summarize: \" + example[\"article\"]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_length=128)\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        preds.append(summary)\n",
    "        refs.append(example[\"highlights\"])\n",
    "    rouge_scores = compute_metrics_rouge(preds, refs)\n",
    "    return rouge_scores\n",
    "\n",
    "# Function to compute perplexity from evaluation loss\n",
    "def compute_perplexity(eval_output):\n",
    "    loss = eval_output[\"eval_loss\"]\n",
    "    return round(torch.exp(torch.tensor(loss)).item(), 3)\n",
    "\n",
    "# Evaluate GPT-2 (if trained) -- example usage\n",
    "gpt2_eval_results = trainer_gpt2.evaluate()\n",
    "gpt2_perplexity = compute_perplexity(gpt2_eval_results)\n",
    "rouge_gpt2 = evaluate_gpt2(gpt2_model, gpt2_tokenizer, small_val_dataset)\n",
    "print(\"GPT-2 Perplexity:\", gpt2_perplexity)\n",
    "print(\"GPT-2 ROUGE:\", rouge_gpt2)\n",
    "\n",
    "# BERT evaluation: compute F1 score - sklearn classification report\n",
    "from sklearn.metrics import classification_report\n",
    "def evaluate_bert(model, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for i in range(0, len(dataset), 32):  # Batch size of 32\n",
    "        batch = dataset[i:i+32]\n",
    "        # batch is a dict with keys: input_ids, token_type_ids, attention_mask, labels\n",
    "        # You need to decode input_ids to get the original text if you want to print or use text\n",
    "        # For inference, use input_ids, attention_mask, etc.\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "            \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = news_classifier_model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    true_labels = np.array(dataset[\"labels\"])\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predictions, output_dict=True)\n",
    "    return report\n",
    "# Evaluate on random 100 samples\n",
    "bert_eval_results = evaluate_bert(news_classifier_model, train_news.shuffle(seed=42).select(range(100)))\n",
    "import json\n",
    "print(\"BERT Evaluation Report:\", json.dumps(bert_eval_results, indent=2))\n",
    "\n",
    "# Demonstrate fill-mask prediction with BERT\n",
    "# from transformers import pipeline\n",
    "# fill_mask = pipeline(\"fill-mask\", model=bert_model, tokenizer=bert_tokenizer)\n",
    "# sentence = \"The weather today is [MASK].\"\n",
    "# print(fill_mask(sentence))\n",
    "\n",
    "# T5 evaluation\n",
    "# t5_eval_results = trainer_t5.evaluate()\n",
    "# t5_perplexity = compute_perplexity(t5_eval_results)\n",
    "\n",
    "# def evaluate_t5(model, tokenizer, dataset, num_samples=10):\n",
    "#     model.eval()\n",
    "#     preds, refs = [], []\n",
    "#     for i, example in enumerate(dataset.select(range(num_samples))):\n",
    "#         input_text = \"summarize: \" + example[\"article\"]\n",
    "#         inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "#         with torch.no_grad():\n",
    "#             output_ids = model.generate(**inputs, max_length=128)\n",
    "#         summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "#         preds.append(summary)\n",
    "#         refs.append(example[\"highlights\"])\n",
    "#     rouge_scores = compute_metrics_rouge(preds, refs)\n",
    "#     return rouge_scores\n",
    "\n",
    "# rouge_t5 = evaluate_t5(t5_model, t5_tokenizer, small_val_dataset)\n",
    "# print(\"T5 Perplexity:\", t5_perplexity)\n",
    "# print(\"T5 ROUGE:\", rouge_t5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8a2bb",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis and discussion\n",
    "\n",
    "After fineâ€‘tuning the models and running the evaluation routines, you should fill in a comparison of the results.  Typical observations include:\n",
    "\n",
    "- **Decoderâ€‘only (GPTâ€‘2):** GPTâ€‘2 fineâ€‘tuned on a summarization corpus learns to generate coherent summaries.  Its perplexity should decrease significantly compared with the preâ€‘trained model, and ROUGE scores should improve.  Because GPTâ€‘2 has no separate encoder, it must memorize how to map the input prompt to the desired output, which can make training less sampleâ€‘efficient for conditional tasks.  However, at inference time GPTâ€‘2 generates outputs quickly via a single decoder.\n",
    "\n",
    "- **Encoderâ€‘only (BERT):** BERT excels at understanding tasks but struggles with generative tasks.  MLM fineâ€‘tuning improves its perplexity on the articleâ€‘summary text, but it cannot generate full summaries.  The `fillâ€‘mask` pipeline can fill individual tokens, but the lack of an autoâ€‘regressive decoder makes longâ€‘form generation impractical.  This illustrates why encoderâ€‘only architectures are not suited for freeâ€‘form text generation.\n",
    "\n",
    "- **Encoderâ€‘decoder (T5):** T5 is designed for textâ€‘toâ€‘text tasks and typically achieves the best summarization scores among the three models when fineâ€‘tuned properly.  Its separate encoder compresses the input, and the decoder generates output conditioned on the encoded context.  T5 often yields higher ROUGE scores and lower perplexity than GPTâ€‘2 on summarization because the architecture explicitly models conditional generation.  The tradeâ€‘off is increased computational cost due to the encoder and decoder.\n",
    "\n",
    "### Chainâ€‘ofâ€‘thought (CoT) reasoning\n",
    "\n",
    "Chainâ€‘ofâ€‘thought reasoning refers to models generating intermediate reasoning steps before arriving at a final answer.  Decoderâ€‘only models (like GPTâ€‘2 and GPTâ€‘3) naturally support CoT prompting because they generate text token by token.  Encoderâ€‘decoder models like T5 can also perform CoT when prompted appropriately (e.g. instructing the model to \"think step by step\").  Encoderâ€‘only models lack a decoding mechanism and therefore are not directly applicable to CoT generation.  In practice, CoT reasoning quality improves with larger models and more sophisticated training (e.g. instructionâ€‘tuning or reinforcement learning with human feedback), which are beyond the scope of this introductory exercise.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we implemented and compared three transformer architectures on a common summarization task using the CNN/DailyMail dataset.  We demonstrated how to fineâ€‘tune a decoderâ€‘only model (GPTâ€‘2), an encoderâ€‘only model (BERT), and an encoderâ€‘decoder model (T5).  The code illustrated data preprocessing, training setups, and evaluation routines using ROUGE and perplexity metrics.  While only small subsets of the dataset were used for demonstration purposes, you should expand the training data and adjust hyperparameters for a thorough experiment.  The analysis underscores the strengths and limitations of each architecture and highlights why encoderâ€‘decoder models are generally preferred for conditional text generation tasks like summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81364b3c",
   "metadata": {},
   "source": [
    "# AssignmentÂ 2: Transformer Architecture Exercise\n",
    "Use this notebook as a starting point and expand on your understanding of transformer models by completing the following structured tasks. You are encouraged to experiment, analyze, and critically reflect on your findings in your report.\n",
    "\n",
    "## Part 1: Model Training & Implementation\n",
    "### 1. Dataset Preparation\n",
    "- Choose one standard text dataset suitable for generative tasks. Options include:\n",
    "  - CNN/DailyMail â†’ summarization\n",
    "  - WikiText-2 â†’ language modeling (text generation)\n",
    "  - SQuAD v1.1 â†’ question answering\n",
    "- Briefly describe why you selected this dataset and what task youâ€™ll evaluate (summarization, QA, or text generation).\n",
    "- Show how you preprocessed the data (tokenization, train/val split, max length, etc.).\n",
    "\n",
    "### 2. Model Implementation\n",
    "\n",
    "Implement and train the following:\n",
    "- Decoder-only model (GPT-style): e.g., GPT-2 small from Hugging Face.\n",
    "- Encoder-only model (BERT-style): e.g., BERT-base, used for masked-language-modeling or extractive QA/summarization.\n",
    "- Encoder-decoder model (T5-style): e.g., T5-small, trained for the same dataset/task as the other two.\n",
    "\n",
    "### 3. Training Documentation\n",
    "\n",
    "- Document your training setup (batch size, learning rate, optimizer, epochs, hardware).\n",
    "- Save a few training/validation loss curves or logs to show how training progressed.\n",
    "- Mention any difficulties you faced and how you addressed them (e.g., memory limits, convergence).\n",
    "\n",
    "## Part 2: Evaluation & Analysis\n",
    "\n",
    "### 4. Performance Evaluation\n",
    "\n",
    "- Evaluate all three models on the same task.\n",
    "- Report results using at least two metrics:\n",
    "  - Text generation/summarization: BLEU, ROUGE, perplexity\n",
    "  - Question answering: F1, Exact Match (EM), BLEU\n",
    "- Include 1â€“2 sample outputs per model to illustrate qualitative differences.\n",
    "\n",
    "### 5. Comparative Discussion\n",
    "\n",
    "- Compare the strengths and weaknesses of each architecture on your chosen task.\n",
    "- Suggested angles:\n",
    "\n",
    "  - Decoder-only: fluent text generation, but weaker at bidirectional context.\n",
    "  - Encoder-only: strong understanding of context, but not designed for open generation.\n",
    "  - Encoder-decoder: flexible, strong on conditional generation tasks (summarization, QA).\n",
    "\n",
    "- Which model seemed easiest to fine-tune?\n",
    "- Which produced the best outputs on your dataset?\n",
    "- Which was the most efficient (speed, memory)?\n",
    "\n",
    "### 6. Reflections on Applicability\n",
    "\n",
    "- In what real-world scenarios would you prefer each architecture?\n",
    "- Briefly note whether you think CoT reasoning would have helped these models if you had added it (conceptual discussion onlyâ€”no experiments required)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
