{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888dcbdc",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 2: Transformer Architecture Exercise\n",
    "\n",
    "This notebook serves as a reference implementation for **Assignment 2** of the generative AI course.  The goal is to compare three prominent transformer architectures—**decoder‑only**, **encoder‑only**, and **encoder‑decoder**—on a common generative task.  The assignment requires training each architecture on the same dataset, evaluating their performance with common metrics, and analysing the implications of architectural differences on generative tasks and chain‑of‑thought reasoning.\n",
    "\n",
    "## Dataset selection\n",
    "\n",
    "For this exercise we use the **CNN/DailyMail** summarisation dataset (version `3.0.0`) from Hugging Face’s `datasets` library.  The dataset comprises news articles paired with human‑written summaries; each article–summary pair provides a natural input/output example for a generative model.  Because the data are already split into training/validation/test splits and are widely used for abstractive summarisation research, this dataset is appropriate for comparing generative architectures.  Although `WikiText` could be used for language modelling tasks, summarisation requires models to generate structured output given an input, which better illustrates differences between decoder‑only, encoder‑only, and encoder‑decoder designs.  For compute efficiency in this notebook we subsample the dataset (e.g. a few hundred training examples) rather than using the full corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb4b15",
   "metadata": {},
   "source": [
    "\n",
    "## Overview of transformer architectures\n",
    "\n",
    "We train three different transformer models:\n",
    "\n",
    "* **Decoder‑only (GPT‑style):** These models consist of stacked self‑attention blocks in which each token can attend only to previous tokens (causal masking).  We use `GPT‑2` as the base model and fine‑tune it to generate a summary from an article.  Because GPT‑2 is a pure language model, we construct input prompts of the form `\"summarize: <article>\"` and train the model to predict the target summary.  During training we mask out the prompt part of the input so that the loss is computed only on the summary tokens.\n",
    "\n",
    "* **Encoder‑only (BERT‑style):** Encoder‑only models such as `BERT` learn bi‑directional contextual representations using masked language modelling (MLM).  They are not inherently generative; they excel at understanding tasks (e.g. classification, token classification).  For a fair comparison on generative tasks we fine‑tune BERT on the same corpus using MLM, combining article and summary text into a single sequence.  At evaluation time we assess perplexity and use the `fill‑mask` capability to approximate generation.  This highlights BERT’s limitations on tasks requiring free‑form generation.\n",
    "\n",
    "* **Encoder‑decoder (T5‑style):** Models like `T5` encode the input sequence with an encoder and decode the output sequence with a separate decoder.  They can perform a wide range of text‑to‑text tasks, including summarisation and question answering.  We fine‑tune `T5‑small` on the CNN/DailyMail dataset using the standard prefix `\"summarize: \"` in the input to indicate the task.  During evaluation we compute ROUGE metrics on generated summaries.\n",
    "\n",
    "The following sections implement data loading, preprocessing, model fine‑tuning, and evaluation for each architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d888988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "from transformers import logging\n",
    "\n",
    "# Silence warnings for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542eddf5",
   "metadata": {},
   "source": [
    "\n",
    "## Load and inspect the dataset\n",
    "\n",
    "We load the CNN/DailyMail dataset using the Hugging Face `datasets` library.  To accelerate training for demonstration purposes we take a small subset of the training and validation sets (e.g. 500 training examples and 100 validation examples).  Each record contains two fields:\n",
    "\n",
    "* `\"article\"`: the news article text (input).\n",
    "* `\"highlights\"`: the human‑written summary (target).\n",
    "\n",
    "Below we load the dataset, inspect a few examples, and create the smaller subsets used for fine‑tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c7d3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': \"By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide poisoning from a cooker. Tragic: The inquests have opened into the deaths of three members of the same family who were found in their static caravan last weekend. John and Audrey Cook are pictured . Awful: The family died following carbon monoxide poisoning at this caravan at the Tremarle Home Park in Camborne, Cornwall . It is also believed there was no working carbon monoxide detector in the static caravan. Cornwall Fire and Rescue Service said this would have resulted in the three being unconscious 'within minutes', . A spokesman for Cornwall coroner Dr Emma Carlyon confirmed the inquests were opened and adjourned yesterday afternoon. They will resume at a later date. Devon and Cornwall Police confirmed on Monday that carbon monoxide poisoning had been established as the cause of death. A police spokesman said the source of the poisoning was 'believed to be from incorrect operation of the gas cooker'. Poisoning: This woman left flowers outside the caravan following the deaths. It has emerged that the trio would have been unconscious 'within minutes' Touching: This tribute was left outside the caravan following news of the deaths . Early readings from experts at the site revealed a potentially lethal level of carbon monoxide present within the caravan at the time it was taken, shortly after the discovery of the bodies. Friends and neighbours have paid tribute to the trio. One . neighbour, Sonya Owen, 53, said: 'It's very distressing. I knew the . daughter, she was living her with her mum and dad. Everybody is really . upset.' Margaret Holmes, 65, who lived near the couple and their . daughter, said: 'They had lived here for around 40 years and they kept . themselves to themselves. 'I just can’t believe this has . happened, it is so sad and I am so shocked, I think we all are, you just . don’t expect this sort of thing to happen on your doorstep. 'Everyone will miss them, we used to chat a lot when we were both in the garden. 'I would just like to send my condolences to their family, I can’t imagine what they’re going through.' Nic Clark, 52, who was good friends with daughter Maureen, added: 'They were a lovely kind family, a great trio. 'Maureen . used to go out and walk her dog, a little Jack Russell, it is so sad . what has happened, I understand the dog went with them. 'They . will be sorely missed and I think everyone is just in shock at the . moment, I would like to send my condolences to the Cook family.'\", 'highlights': 'John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'id': '08cf276c9eadb638e0c7fdc83ce0229c8af5d09b'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
    "\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9c398",
   "metadata": {},
   "source": [
    "\n",
    "## Decoder‑only model: GPT‑2 fine‑tuning\n",
    "\n",
    "A decoder‑only transformer must learn to generate a summary given an input article.  We use a prompt‑based approach: the input text has the form `\"summarize: <article>\"`, and the model is trained to produce the summary tokens.  To prevent the model from learning to predict the prompt tokens, we mask the loss on the prompt portion of the sequence (by setting corresponding labels to `-100`).\n",
    "\n",
    "We use the `GPT‑2` tokenizer and model from Hugging Face.  Because GPT‑2 lacks a padding token by default, we add a pad token equal to the end‑of‑text token.  We then tokenize the inputs and construct labels accordingly.  The function below performs these steps and is mapped over the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6c9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1220.82 examples/s]\n",
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1186.44 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels']) 512\n",
      "summarize: By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Cam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0862232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs and Labels dimensions for first 10 samples:\n",
      "Sample 0: Input IDs length = 512, Labels length = 512\n",
      "Sample 1: Input IDs length = 512, Labels length = 512\n",
      "Sample 2: Input IDs length = 512, Labels length = 512\n",
      "Sample 3: Input IDs length = 512, Labels length = 512\n",
      "Sample 4: Input IDs length = 512, Labels length = 512\n",
      "Sample 5: Input IDs length = 512, Labels length = 512\n",
      "Sample 6: Input IDs length = 512, Labels length = 512\n",
      "Sample 7: Input IDs length = 512, Labels length = 512\n",
      "Sample 8: Input IDs length = 512, Labels length = 512\n",
      "Sample 9: Input IDs length = 512, Labels length = 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs and Labels dimensions for first 10 samples:\")\n",
    "for i in range(10):\n",
    "    print(f\"Sample {i}: Input IDs length = {len(train_gpt2[i]['input_ids'])}, Labels length = {len(train_gpt2[i]['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67ece",
   "metadata": {},
   "source": [
    "\n",
    "### GPT‑2 training configuration\n",
    "\n",
    "We use the Hugging Face `Trainer` API to fine‑tune the GPT‑2 model.  A `DataCollatorForLanguageModeling` automatically pads the inputs and labels and performs dynamic masking where appropriate (although in our custom loss masking we already set `-100` values).  The training arguments below specify a small number of epochs and batch sizes for illustration; adjust these for a full training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a0222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 68.9499, 'train_samples_per_second': 7.252, 'train_steps_per_second': 0.914, 'train_loss': 3.2406868102058533, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=3.2406868102058533, metrics={'train_runtime': 68.9499, 'train_samples_per_second': 7.252, 'train_steps_per_second': 0.914, 'train_loss': 3.2406868102058533, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define data collator\n",
    "data_collator_gpt2 = DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False)\n",
    "\n",
    "# Load the GPT-2 model\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Training arguments\n",
    "training_args_gpt2 = TrainingArguments(\n",
    "    output_dir=\"./gpt2-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],  # disable logging to wandb\n",
    ")\n",
    "\n",
    "# Create Trainer for GPT-2\n",
    "trainer_gpt2 = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args_gpt2,\n",
    "    train_dataset=train_gpt2,\n",
    "    eval_dataset=val_gpt2,\n",
    "    data_collator=data_collator_gpt2,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train; training can take several minutes even on small subsets\n",
    "trainer_gpt2.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f5d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary: summarize: The quick brown fox jumps over the lazy dog. The fox is a little bit of a dog, but it's not a dog. It's a little bit of a dog, but it's not a dog. It's a\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample input\n",
    "sample_input = \"summarize: The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = gpt2_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "outputs = gpt2_model.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
    "print(\"Generated summary:\", gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16871507",
   "metadata": {},
   "source": [
    "## What do you observe in the output?\n",
    "1. Can you postprocess the output so that it only starts printing after the input sequence?\n",
    "2. Can you iterate and improve the summary quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29cc30",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder‑only model: BERT fine‑tuning\n",
    "\n",
    "BERT uses bi‑directional self‑attention and is optimised for understanding rather than generation.  To apply BERT on our corpus we fine‑tune it using the **masked language modelling (MLM)** objective.  We concatenate the article and its summary into a single sequence and randomly mask tokens using `DataCollatorForLanguageModeling`.  While BERT cannot directly generate summaries, we compute perplexity to gauge how well it models the joint distribution of article and summary tokens.  At evaluation we also demonstrate how to use the `fill-mask` pipeline to generate single masked words as an illustration of BERT’s generative limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bee9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 samples of the dataset:\n",
      "small_train_dataset[0]: dict_keys(['article', 'highlights', 'id'])\n",
      "\t\tsmall_train_dataset[0]: By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a sta... \n",
      "\t->\n",
      "\t\tJohn and .\n",
      "Audrey Cook were discovered alongside their daughter, Maureen .\n",
      "They were found at Tremarle Home Park in Cornwall .\n",
      "Investigators say the three died of carbon monoxide .\n",
      "poisoning .\n",
      "\t\tsmall_train_dataset[1]: UNITED NATIONS (CNN) -- A rare meeting of U.N. Security Council heads of state, led for the first time by a U.S. president, adopted a resolution focus... \n",
      "\t->\n",
      "\t\tNEW: Libya can serve as example of cooperation, White House spokesman says .\n",
      "Resolution calls for preventing nuclear weapons from being stolen, used by military .\n",
      "Obama, Russian President Dimitry Medvedev working to reduce stockpiles .\n",
      "Venezuelan president Hugo Chavez on \"Larry King Live\" tonight, 9 ET .\n",
      "\t\tsmall_train_dataset[2]: Cover-up: Former Archbishop Lord Hope allowed a paedophile priest to escape punishment for sex crimes, a judge's report claims . A former archbishop w... \n",
      "\t->\n",
      "\t\tVery Reverend Robert Waddington sexually abused choirboys for decades .\n",
      "Inquiry into the abuse has slammed Lord Hope, former Archbishop of York .\n",
      "Report claims he was made aware of misconduct 19 times but did not act .\n",
      "Despite this - he still holds influential post and sits in the House of Lords .\n",
      "\t\tsmall_train_dataset[3]: By . Kristie Lau . PUBLISHED: . 10:48 EST, 14 June 2012 . | . UPDATED: . 11:02 EST, 14 June 2012 . TLC has pulled an episode of Cake Boss from future ... \n",
      "\t->\n",
      "\t\tMonday night's episode showed Buddy Valastro tricking Anthony Bellifemine into thinking that Carmen Carerra, 27, was born as a woman .\n",
      "TLC removed the episode from future screening schedules .\n",
      "\t\tsmall_train_dataset[4]: 'The lamps are going out all over Europe. We shall not see them lit again in our lifetime' Foreign Secretary Sir Edward Grey, August 3, 1914 . Prime M... \n",
      "\t->\n",
      "\t\tPeople asked to turn out lights for hour between 10 and 11pm tomorrow .\n",
      "Gesture is in remembrance of those killed in the First World War .\n",
      "Tower Bridge and 10 Downing Street will also extinguish all but one light .\n",
      "\t\tsmall_train_dataset[5]: Roy Hodgson has come under fire for making public Raheem Sterling's admission that he was feeling tired ahead of England's Euro 2016 qualifier with Es... \n",
      "\t->\n",
      "\t\tRoy Hodgson revealed that Raheem Sterling had told him he was feeling fatigued prior to England's Euro 2016 qualifier with Estonia .\n",
      "Sterling was dropped from the starting line-up in place of Adam Lallana .\n",
      "The Liverpool man later came on as a substitute as England won 1-0 .\n",
      "Hodgson was criticised by Sky Sports pundits Jamie Redknapp and Jamie Carragher for making Sterling's admission of tiredness public .\n",
      "Wayne Rooney's free-kick preserved England's perfect start to qualifying .\n",
      "\t\tsmall_train_dataset[6]: Every frontline police officer should be offered a Taser to help fight the threat from lone-wolf terrorists, a police leader declared yesterday. Steve... \n",
      "\t->\n",
      "\t\tCalls for every police officer to be offered a Taser to fight terrorist threat .\n",
      "Police Federation set to vote on giving all frontline officers training .\n",
      "Terrorist threat level for police was raised to severe after Paris attacks .\n",
      "\t\tsmall_train_dataset[7]: By . Dan Bloom . It's one very, very small step for man - but a giant leap for antkind. Around 800 common ants, usually found pattering across pavemen... \n",
      "\t->\n",
      "\t\t800 common pavement ants are now living on International Space Station .\n",
      "Scientists will examine how they work together in low gravity to find food .\n",
      "The ants' methods can then be copied to develop 'intelligent' search robots .\n",
      "\t\tsmall_train_dataset[8]: By . Jill Reilly . PUBLISHED: . 09:26 EST, 14 June 2012 . | . UPDATED: . 13:19 EST, 14 June 2012 . A pensioner died from fatal injuries after being hi... \n",
      "\t->\n",
      "\t\tDavid Wilcockson, 71, was bowling at a ground in  Cranleigh, Surrey when the ball struck him on the head .\n",
      "Died in hospital on June 1 after 13 days in a coma .\n",
      "He was the longest-serving member of the Old Dorkinians, joining the club in 1959 .\n",
      "The pensioner had set himself a target of 3,000 wickets - and died just 101 short .\n",
      "\t\tsmall_train_dataset[9]: Workers digging an underground garage for a new hotel  recently struck something big about 30 feet below the surface. This week they uncovered it - a ... \n",
      "\t->\n",
      "\t\tThe boulder was found by a construction crew in Everett, Washington .\n",
      "The rock is bigger than an SUV and close to 19 feet long .\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 samples of the dataset\n",
    "print(\"First 10 samples of the dataset:\")\n",
    "print(\"small_train_dataset[0]:\", small_train_dataset[0].keys())\n",
    "for i in range(10):\n",
    "    print(f\"\\t\\tsmall_train_dataset[{i}]: {small_train_dataset[i]['article'][:150]}... \\n\\t->\\n\\t\\t{small_train_dataset[i]['highlights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba715f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1259.09 examples/s]\n",
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2655, 'grad_norm': 9.493582725524902, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 1.8074514865875244, 'eval_runtime': 1.6633, 'eval_samples_per_second': 60.12, 'eval_steps_per_second': 15.03, 'epoch': 0.8}\n",
      "{'train_runtime': 39.65, 'train_samples_per_second': 12.61, 'train_steps_per_second': 3.153, 'train_loss': 2.230162872314453, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=2.230162872314453, metrics={'train_runtime': 39.65, 'train_samples_per_second': 12.61, 'train_steps_per_second': 3.153, 'train_loss': 2.230162872314453, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model for BERT\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define preprocessing: combine article and summary\n",
    "\n",
    "def preprocess_bert(examples):\n",
    "    concatenated_texts = [\n",
    "        art + \" \" + summ \n",
    "        for art, summ in zip(examples[\"article\"], examples[\"highlights\"])\n",
    "    ]\n",
    "    model_inputs = bert_tokenizer(concatenated_texts, max_length=512, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_bert = small_train_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_bert = small_val_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Data collator with MLM\n",
    "mlm_probability = 0.15\n",
    "data_collator_bert = DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=True, mlm_probability=mlm_probability)\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "# Training arguments for BERT\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir=\"./bert-mlm\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_bert,\n",
    "    eval_dataset=val_bert,\n",
    "    data_collator=data_collator_bert,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train the BERT model\n",
    "trainer_bert.train()\n",
    "# save the model\n",
    "bert_model.save_pretrained(\"./bert-mlm\")\n",
    "bert_tokenizer.save_pretrained(\"./bert-mlm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49de6626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: torch.Size([1, 12, 30522]) logits\n",
      "News dataset loaded with 1000 samples.\n",
      "{'loss': 0.5652, 'grad_norm': 0.7264517545700073, 'learning_rate': 4.2461538461538465e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.3197597563266754, 'eval_runtime': 10.5176, 'eval_samples_per_second': 95.079, 'eval_steps_per_second': 11.885, 'epoch': 0.8}\n",
      "{'loss': 0.2937, 'grad_norm': 0.40986400842666626, 'learning_rate': 2.7076923076923078e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 0.21455331146717072, 'eval_runtime': 10.5891, 'eval_samples_per_second': 94.437, 'eval_steps_per_second': 11.805, 'epoch': 1.6}\n",
      "{'loss': 0.2386, 'grad_norm': 0.39017254114151, 'learning_rate': 1.1692307692307693e-05, 'epoch': 2.4}\n",
      "{'eval_loss': 0.1729050725698471, 'eval_runtime': 10.611, 'eval_samples_per_second': 94.241, 'eval_steps_per_second': 11.78, 'epoch': 2.4}\n",
      "{'train_runtime': 77.3114, 'train_samples_per_second': 38.804, 'train_steps_per_second': 4.851, 'train_loss': 0.33428941345214847, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.33428941345214847, metrics={'train_runtime': 77.3114, 'train_samples_per_second': 38.804, 'train_steps_per_second': 4.851, 'train_loss': 0.33428941345214847, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of bert model for sample input\n",
    "sample_input = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = bert_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "outputs = bert_model(**inputs)\n",
    "print(f\"Sample {i}: {outputs.logits.shape} logits\")       \n",
    "\n",
    "# Freeze the BERT model and train a classifier on top\n",
    "# load a news sentiment classification dataset\n",
    "from datasets import load_dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "# Use the trained model as a feature extractor and add a classification head\n",
    "def get_news_classifier_model():\n",
    "    \"\"\"\n",
    "    load the trained BERT model and add a classification head\n",
    "    \"\"\"\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"./bert-mlm\") # Load the trained BERT model\n",
    "    # Freeze the BERT model\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Add a classification head\n",
    "    model.classifier = torch.nn.Linear(model.config.hidden_size, 4)  # 4 classes for AG News\n",
    "    return model\n",
    "\n",
    "news_classifier_model = get_news_classifier_model()\n",
    "# Training arguments for news classifier\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=False),\n",
    ")\n",
    "# Uncomment the line below to train the news classifier\n",
    "news_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e0b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News dataset loaded with 1000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3949.09 examples/s]\n",
      "C:\\Users\\onurb\\AppData\\Local\\Temp\\ipykernel_6872\\2350930871.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  news_trainer = Trainer(\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3949.09 examples/s]\n",
      "C:\\Users\\onurb\\AppData\\Local\\Temp\\ipykernel_6872\\2350930871.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  news_trainer = Trainer(\n",
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.143100</td>\n",
       "      <td>0.632532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.645700</td>\n",
       "      <td>0.831645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.446590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.645629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.423987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.396483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.403947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>0.299648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.196146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.169152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.180398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.106801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.097870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>0.099259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.092184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample news input: Breaking news: The stock market crashes as investors panic.\n",
      "Predicted class: 2\n",
      "Available classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Print sample output of the news classifier\n",
    "# The previous approach used AutoModelForMaskedLM, which is not suitable for classification.\n",
    "# Instead, use AutoModelForSequenceClassification for news classification.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a BERT model with a classification head\n",
    "news_classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    bert_model_name, num_labels=4\n",
    ").to(device)\n",
    "\n",
    "# Training arguments for news classifier remain the same\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# Load the AG News dataset\n",
    "from datasets import load_dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    tokenizer=bert_tokenizer,\n",
    ")\n",
    "# Uncomment the line below to train the news classifier\n",
    "news_trainer.train()\n",
    "\n",
    "# Print sample output of the news classifier\n",
    "sample_news_input = \"Breaking news: The stock market crashes as investors panic.\"\n",
    "inputs = bert_tokenizer(sample_news_input, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = news_classifier_model(**inputs)\n",
    "pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "print(f\"Sample news input: {sample_news_input}\")\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(\"Available classes:\", news_dataset.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0127fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[206   0   2   4]\n",
      " [  0 142   0   0]\n",
      " [  2   1 159  12]\n",
      " [  0   0   1 471]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# Get predictions for the entire training set\n",
    "predictions = []\n",
    "for i in range(0, len(train_news), 32):  # Batch size of 32\n",
    "    batch = train_news[i:i+32]\n",
    "    # batch is a dict with keys: input_ids, token_type_ids, attention_mask, labels\n",
    "    # You need to decode input_ids to get the original text if you want to print or use text\n",
    "    # For inference, use input_ids, attention_mask, etc.\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "        \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = news_classifier_model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    predictions.extend(preds)\n",
    "# Get true labels\n",
    "true_labels = np.array(batch[\"labels\"] for batch in train_news)  # This line should be fixed\n",
    "# Instead, collect all labels from train_news:\n",
    "true_labels = np.array(train_news[\"labels\"])\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=news_dataset.features[\"label\"].names,\n",
    "            yticklabels=news_dataset.features[\"label\"].names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for News Classifier\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ee761",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder‑decoder model: T5 fine‑tuning\n",
    "\n",
    "T5 is a text‑to‑text model that uses a separate encoder and decoder.  It naturally handles generative tasks such as summarisation.  We prepend the prefix `\"summarize: \"` to each article, then tokenize the input and the summary separately.  A `DataCollatorForSeq2Seq` takes care of padding the inputs and shifting the decoder labels.  During evaluation we use greedy decoding to produce summaries and compute ROUGE scores against the reference summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14ed723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': \"By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide poisoning from a cooker. Tragic: The inquests have opened into the deaths of three members of the same family who were found in their static caravan last weekend. John and Audrey Cook are pictured . Awful: The family died following carbon monoxide poisoning at this caravan at the Tremarle Home Park in Camborne, Cornwall . It is also believed there was no working carbon monoxide detector in the static caravan. Cornwall Fire and Rescue Service said this would have resulted in the three being unconscious 'within minutes', . A spokesman for Cornwall coroner Dr Emma Carlyon confirmed the inquests were opened and adjourned yesterday afternoon. They will resume at a later date. Devon and Cornwall Police confirmed on Monday that carbon monoxide poisoning had been established as the cause of death. A police spokesman said the source of the poisoning was 'believed to be from incorrect operation of the gas cooker'. Poisoning: This woman left flowers outside the caravan following the deaths. It has emerged that the trio would have been unconscious 'within minutes' Touching: This tribute was left outside the caravan following news of the deaths . Early readings from experts at the site revealed a potentially lethal level of carbon monoxide present within the caravan at the time it was taken, shortly after the discovery of the bodies. Friends and neighbours have paid tribute to the trio. One . neighbour, Sonya Owen, 53, said: 'It's very distressing. I knew the . daughter, she was living her with her mum and dad. Everybody is really . upset.' Margaret Holmes, 65, who lived near the couple and their . daughter, said: 'They had lived here for around 40 years and they kept . themselves to themselves. 'I just can’t believe this has . happened, it is so sad and I am so shocked, I think we all are, you just . don’t expect this sort of thing to happen on your doorstep. 'Everyone will miss them, we used to chat a lot when we were both in the garden. 'I would just like to send my condolences to their family, I can’t imagine what they’re going through.' Nic Clark, 52, who was good friends with daughter Maureen, added: 'They were a lovely kind family, a great trio. 'Maureen . used to go out and walk her dog, a little Jack Russell, it is so sad . what has happened, I understand the dog went with them. 'They . will be sorely missed and I think everyone is just in shock at the . moment, I would like to send my condolences to the Cook family.'\", 'highlights': 'John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'id': '08cf276c9eadb638e0c7fdc83ce0229c8af5d09b'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1015.04 examples/s]\n",
      "\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\onurb\\AppData\\Local\\Temp\\ipykernel_6872\\3507327657.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_t5 = Trainer(\n",
      "C:\\Users\\onurb\\AppData\\Local\\Temp\\ipykernel_6872\\3507327657.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_t5 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=2.4141407315693204, metrics={'train_runtime': 37.7167, 'train_samples_per_second': 13.257, 'train_steps_per_second': 1.67, 'total_flos': 67661913194496.0, 'train_loss': 2.4141407315693204, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "def preprocess_t5(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = t5_tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = t5_tokenizer(examples[\"highlights\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_t5 = small_train_dataset.map(preprocess_t5, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_t5 = small_val_dataset.map(preprocess_t5, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Data collator for seq2seq tasks\n",
    "data_collator_t5 = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model_name)\n",
    "\n",
    "# Load T5 model\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_name)\n",
    "\n",
    "# Training arguments for T5\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir=\"./t5-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=train_t5,\n",
    "    eval_dataset=val_t5,\n",
    "    data_collator=data_collator_t5,\n",
    "    tokenizer=t5_tokenizer,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train the T5 model\n",
    "trainer_t5.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1ffa7",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation\n",
    "\n",
    "After fine‑tuning the models (training steps are commented out by default), we evaluate them on the validation subset. Different metrics are appropriate for each architecture:\n",
    "\n",
    "* **GPT‑2** (decoder‑only): We generate summaries using greedy decoding and compute ROUGE metrics (ROUGE‑1, ROUGE‑2, ROUGE‑L). We also compute perplexity using the loss returned by the trainer.\n",
    "\n",
    "* **BERT** (encoder‑only): BERT is not designed to generate full sequences; instead we use it for downstream tasks such as text classification. For a classification scenario, the evaluation metrics are typically confusion matrix and F1-score.\n",
    "\n",
    "* **T5** (encoder‑decoder): We generate summaries using greedy decoding and compute ROUGE metrics.  Perplexity is computed similarly to GPT‑2 by exponentiating the validation loss.\n",
    "\n",
    "The code below demonstrates evaluation routines for each model. Running these functions requires trained models; if you skipped training above, the evaluation will use the pre‑trained weights and therefore will not yield good summarization quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4253cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation Report: {\n",
      "  \"0\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 0.9523809523809523,\n",
      "    \"f1-score\": 0.975609756097561,\n",
      "    \"support\": 21.0\n",
      "  },\n",
      "  \"1\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 1.0,\n",
      "    \"support\": 16.0\n",
      "  },\n",
      "  \"2\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 1.0,\n",
      "    \"support\": 18.0\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"precision\": 0.9782608695652174,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 0.989010989010989,\n",
      "    \"support\": 45.0\n",
      "  },\n",
      "  \"accuracy\": 0.99,\n",
      "  \"macro avg\": {\n",
      "    \"precision\": 0.9945652173913043,\n",
      "    \"recall\": 0.9880952380952381,\n",
      "    \"f1-score\": 0.9911551862771375,\n",
      "    \"support\": 100.0\n",
      "  },\n",
      "  \"weighted avg\": {\n",
      "    \"precision\": 0.9902173913043478,\n",
      "    \"recall\": 0.99,\n",
      "    \"f1-score\": 0.9899329938354329,\n",
      "    \"support\": 100.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define ROUGE metric\n",
    "evaluate_rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics_rouge(preds, refs):\n",
    "    # Compute ROUGE scores; use newline separation between sentences in each text\n",
    "    result = evaluate_rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "# Function to generate summaries with GPT-2\n",
    "def evaluate_gpt2(model, tokenizer, dataset, num_samples=10):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for i, example in enumerate(dataset.select(range(num_samples))):\n",
    "        prompt = \"summarize: \" + example[\"article\"]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_length=128)\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        preds.append(summary)\n",
    "        refs.append(example[\"highlights\"])\n",
    "    rouge_scores = compute_metrics_rouge(preds, refs)\n",
    "    return rouge_scores\n",
    "\n",
    "# Function to compute perplexity from evaluation loss\n",
    "def compute_perplexity(eval_output):\n",
    "    loss = eval_output[\"eval_loss\"]\n",
    "    return round(torch.exp(torch.tensor(loss)).item(), 3)\n",
    "\n",
    "# Evaluate GPT-2 (if trained) -- example usage\n",
    "gpt2_eval_results = trainer_gpt2.evaluate()\n",
    "gpt2_perplexity = compute_perplexity(gpt2_eval_results)\n",
    "rouge_gpt2 = evaluate_gpt2(gpt2_model, gpt2_tokenizer, small_val_dataset)\n",
    "print(\"GPT-2 Perplexity:\", gpt2_perplexity)\n",
    "print(\"GPT-2 ROUGE:\", rouge_gpt2)\n",
    "\n",
    "# BERT evaluation: compute F1 score - sklearn classification report\n",
    "from sklearn.metrics import classification_report\n",
    "def evaluate_bert(model, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for i in range(0, len(dataset), 32):  # Batch size of 32\n",
    "        batch = dataset[i:i+32]\n",
    "        # batch is a dict with keys: input_ids, token_type_ids, attention_mask, labels\n",
    "        # You need to decode input_ids to get the original text if you want to print or use text\n",
    "        # For inference, use input_ids, attention_mask, etc.\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "            \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = news_classifier_model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    true_labels = np.array(dataset[\"labels\"])\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predictions, output_dict=True)\n",
    "    return report\n",
    "# Evaluate on random 100 samples\n",
    "bert_eval_results = evaluate_bert(news_classifier_model, train_news.shuffle(seed=42).select(range(100)))\n",
    "import json\n",
    "print(\"BERT Evaluation Report:\", json.dumps(bert_eval_results, indent=2))\n",
    "\n",
    "# Demonstrate fill-mask prediction with BERT\n",
    "# from transformers import pipeline\n",
    "# fill_mask = pipeline(\"fill-mask\", model=bert_model, tokenizer=bert_tokenizer)\n",
    "# sentence = \"The weather today is [MASK].\"\n",
    "# print(fill_mask(sentence))\n",
    "\n",
    "# T5 evaluation\n",
    "# t5_eval_results = trainer_t5.evaluate()\n",
    "# t5_perplexity = compute_perplexity(t5_eval_results)\n",
    "\n",
    "# def evaluate_t5(model, tokenizer, dataset, num_samples=10):\n",
    "#     model.eval()\n",
    "#     preds, refs = [], []\n",
    "#     for i, example in enumerate(dataset.select(range(num_samples))):\n",
    "#         input_text = \"summarize: \" + example[\"article\"]\n",
    "#         inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "#         with torch.no_grad():\n",
    "#             output_ids = model.generate(**inputs, max_length=128)\n",
    "#         summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "#         preds.append(summary)\n",
    "#         refs.append(example[\"highlights\"])\n",
    "#     rouge_scores = compute_metrics_rouge(preds, refs)\n",
    "#     return rouge_scores\n",
    "\n",
    "# rouge_t5 = evaluate_t5(t5_model, t5_tokenizer, small_val_dataset)\n",
    "# print(\"T5 Perplexity:\", t5_perplexity)\n",
    "# print(\"T5 ROUGE:\", rouge_t5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8a2bb",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis and discussion\n",
    "\n",
    "After fine‑tuning the models and running the evaluation routines, you should fill in a comparison of the results.  Typical observations include:\n",
    "\n",
    "- **Decoder‑only (GPT‑2):** GPT‑2 fine‑tuned on a summarization corpus learns to generate coherent summaries.  Its perplexity should decrease significantly compared with the pre‑trained model, and ROUGE scores should improve.  Because GPT‑2 has no separate encoder, it must memorize how to map the input prompt to the desired output, which can make training less sample‑efficient for conditional tasks.  However, at inference time GPT‑2 generates outputs quickly via a single decoder.\n",
    "\n",
    "- **Encoder‑only (BERT):** BERT excels at understanding tasks but struggles with generative tasks.  MLM fine‑tuning improves its perplexity on the article‑summary text, but it cannot generate full summaries.  The `fill‑mask` pipeline can fill individual tokens, but the lack of an auto‑regressive decoder makes long‑form generation impractical.  This illustrates why encoder‑only architectures are not suited for free‑form text generation.\n",
    "\n",
    "- **Encoder‑decoder (T5):** T5 is designed for text‑to‑text tasks and typically achieves the best summarization scores among the three models when fine‑tuned properly.  Its separate encoder compresses the input, and the decoder generates output conditioned on the encoded context.  T5 often yields higher ROUGE scores and lower perplexity than GPT‑2 on summarization because the architecture explicitly models conditional generation.  The trade‑off is increased computational cost due to the encoder and decoder.\n",
    "\n",
    "### Chain‑of‑thought (CoT) reasoning\n",
    "\n",
    "Chain‑of‑thought reasoning refers to models generating intermediate reasoning steps before arriving at a final answer.  Decoder‑only models (like GPT‑2 and GPT‑3) naturally support CoT prompting because they generate text token by token.  Encoder‑decoder models like T5 can also perform CoT when prompted appropriately (e.g. instructing the model to \"think step by step\").  Encoder‑only models lack a decoding mechanism and therefore are not directly applicable to CoT generation.  In practice, CoT reasoning quality improves with larger models and more sophisticated training (e.g. instruction‑tuning or reinforcement learning with human feedback), which are beyond the scope of this introductory exercise.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we implemented and compared three transformer architectures on a common summarization task using the CNN/DailyMail dataset.  We demonstrated how to fine‑tune a decoder‑only model (GPT‑2), an encoder‑only model (BERT), and an encoder‑decoder model (T5).  The code illustrated data preprocessing, training setups, and evaluation routines using ROUGE and perplexity metrics.  While only small subsets of the dataset were used for demonstration purposes, you should expand the training data and adjust hyperparameters for a thorough experiment.  The analysis underscores the strengths and limitations of each architecture and highlights why encoder‑decoder models are generally preferred for conditional text generation tasks like summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81364b3c",
   "metadata": {},
   "source": [
    "# Assignment 2: Transformer Architecture Exercise\n",
    "Use this notebook as a starting point and expand on your understanding of transformer models by completing the following structured tasks. You are encouraged to experiment, analyze, and critically reflect on your findings in your report.\n",
    "\n",
    "## Part 1: Model Training & Implementation\n",
    "### 1. Dataset Preparation\n",
    "- Choose one standard text dataset suitable for generative tasks. Options include:\n",
    "  - CNN/DailyMail → summarization\n",
    "  - WikiText-2 → language modeling (text generation)\n",
    "  - SQuAD v1.1 → question answering\n",
    "- Briefly describe why you selected this dataset and what task you’ll evaluate (summarization, QA, or text generation).\n",
    "- Show how you preprocessed the data (tokenization, train/val split, max length, etc.).\n",
    "\n",
    "### 2. Model Implementation\n",
    "\n",
    "Implement and train the following:\n",
    "- Decoder-only model (GPT-style): e.g., GPT-2 small from Hugging Face.\n",
    "- Encoder-only model (BERT-style): e.g., BERT-base, used for masked-language-modeling or extractive QA/summarization.\n",
    "- Encoder-decoder model (T5-style): e.g., T5-small, trained for the same dataset/task as the other two.\n",
    "\n",
    "### 3. Training Documentation\n",
    "\n",
    "- Document your training setup (batch size, learning rate, optimizer, epochs, hardware).\n",
    "- Save a few training/validation loss curves or logs to show how training progressed.\n",
    "- Mention any difficulties you faced and how you addressed them (e.g., memory limits, convergence).\n",
    "\n",
    "## Part 2: Evaluation & Analysis\n",
    "\n",
    "### 4. Performance Evaluation\n",
    "\n",
    "- Evaluate all three models on the same task.\n",
    "- Report results using at least two metrics:\n",
    "  - Text generation/summarization: BLEU, ROUGE, perplexity\n",
    "  - Question answering: F1, Exact Match (EM), BLEU\n",
    "- Include 1–2 sample outputs per model to illustrate qualitative differences.\n",
    "\n",
    "### 5. Comparative Discussion\n",
    "\n",
    "- Compare the strengths and weaknesses of each architecture on your chosen task.\n",
    "- Suggested angles:\n",
    "\n",
    "  - Decoder-only: fluent text generation, but weaker at bidirectional context.\n",
    "  - Encoder-only: strong understanding of context, but not designed for open generation.\n",
    "  - Encoder-decoder: flexible, strong on conditional generation tasks (summarization, QA).\n",
    "\n",
    "- Which model seemed easiest to fine-tune?\n",
    "- Which produced the best outputs on your dataset?\n",
    "- Which was the most efficient (speed, memory)?\n",
    "\n",
    "### 6. Reflections on Applicability\n",
    "\n",
    "- In what real-world scenarios would you prefer each architecture?\n",
    "- Briefly note whether you think CoT reasoning would have helped these models if you had added it (conceptual discussion only—no experiments required)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
