{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fad11b6",
   "metadata": {},
   "source": [
    "\n",
    "# MSIT 3103 · Assignment 3 Hotstart Notebook  \n",
    "**Prompt Engineering Lab with Local Models (Ollama + LLaMA 3.2 1B)**\n",
    "\n",
    "> Use this notebook to run structured prompt‑engineering experiments on a **local** small model (e.g., `llama3.2:1b` via Ollama).  \n",
    "> You will implement Steps 1–4 (Basic → Structured → Few‑shot → CoT), collect outputs, and analyze results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4057ab",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Local Environment Check\n",
    "\n",
    "This notebook assumes you have **[Ollama](https://ollama.com/)** installed locally and the `llama3.2:1b` model available.\n",
    "\n",
    "- Install Ollama: follow your OS‑specific steps from the website.  \n",
    "- Pull the model (in a terminal):  \n",
    "```bash\n",
    "ollama pull llama3.2:1b\n",
    "```\n",
    "- Verify command works (from terminal): `ollama run llama3.2:1b \"hello\"`\n",
    "\n",
    "> If you're executing this notebook on a machine without Ollama, use these cells as **documentation** and run the actual commands on your local machine. You can paste results back here for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af339e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, os, subprocess, time, uuid, textwrap\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "RESULTS_DIR = \"a3_outputs\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def ollama_run(model: str, prompt: str, options: Dict=None, timeout: int = 120) -> str:\n",
    "    \"\"\"Run a prompt through a local Ollama model via subprocess and return the text.\n",
    "\n",
    "    Requires Ollama to be installed and available on PATH.\n",
    "\n",
    "    \"\"\"\n",
    "    cmd = [\"ollama\", \"run\", model]\n",
    "    if options:\n",
    "        # Ollama CLI supports options via the JSON API; here we keep CLI simple.\n",
    "        # For advanced options, consider using `ollama generate -m model -p prompt --options '{...}'` pattern.\n",
    "        pass\n",
    "    proc = subprocess.run(cmd, input=prompt.encode(\"utf-8\"), stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"Ollama error: {proc.stderr.decode('utf-8', errors='ignore')}\")\n",
    "    return proc.stdout.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "@dataclass\n",
    "class Trial:\n",
    "    step: str                 # basic / structured / fewshot / cot\n",
    "    task_id: str              # arbitrary id of the task/question\n",
    "    prompt: str\n",
    "    response: str\n",
    "    meta: Dict\n",
    "\n",
    "def save_trials(trials: List[Trial], path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([asdict(t) for t in trials], f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(trials)} trials to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a48672",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Context Document & Tasks\n",
    "\n",
    "Create/Load a short **context document** (few paragraphs) and a small set of **tasks** to query the model with.  \n",
    "You may replace the placeholder context and tasks below with your own domain (e.g., finance, health, policy, product requirements).\n",
    "\n",
    "> Keep **5–10 tasks** so runs complete quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d85e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTEXT = \"\"\"\n",
    "Acme Coffee is launching a mobile ordering app. Users can customize drinks, save favorites, and schedule pickup.\n",
    "New features include: seasonal drinks, reward points, referral codes, and a barista recommendation assistant.\n",
    "The team tracks weekly KPIs: daily active users (DAU), average order value (AOV), and retention D7.\n",
    "\"\"\".strip()\n",
    "\n",
    "TASKS = [\n",
    "    {\"id\": \"t1\", \"question\": \"Summarize the key features of the Acme Coffee app.\"},\n",
    "    {\"id\": \"t2\", \"question\": \"List three KPIs and explain why they matter.\"},\n",
    "    {\"id\": \"t3\", \"question\": \"Propose two experiments to increase AOV.\"},\n",
    "    {\"id\": \"t4\", \"question\": \"Write a short FAQ answer: 'How do I redeem reward points?'\"}\n",
    "]\n",
    "\n",
    "print(CONTEXT)\n",
    "print(\"\\nTasks:\")\n",
    "for t in TASKS:\n",
    "    print(\"-\", t[\"id\"] + \":\", t[\"question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bea6e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Step 1 — Basic Prompting\n",
    "\n",
    "- Ask direct, simple questions grounded in the context document.\n",
    "- Log correctness, completeness, and clarity for later comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = \"llama3.2:1b\"\n",
    "\n",
    "basic_trials = []\n",
    "for task in TASKS:\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "Context:\n",
    "{CONTEXT}\n",
    "\n",
    "Question:\n",
    "{task['question']}\n",
    "\"\"\".strip()\n",
    "    try:\n",
    "        resp = ollama_run(MODEL, prompt)\n",
    "    except Exception as e:\n",
    "        resp = f\"[ERROR running model locally] {e}\"\n",
    "    basic_trials.append(Trial(step=\"basic\", task_id=task[\"id\"], prompt=prompt, response=resp, meta={}))\n",
    "\n",
    "save_trials(basic_trials, os.path.join(RESULTS_DIR, \"step1_basic.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bf482",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Step 2 — Structured Output Prompts\n",
    "\n",
    "- Instruct the model to output **strict JSON** or bullet lists/tables.\n",
    "- Compare accuracy & reliability with Step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "structured_trials = []\n",
    "for task in TASKS:\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Return the answer in **strict JSON** with keys:\n",
    "- \"answer\": string\n",
    "- \"supporting_facts\": list of short strings derived from the context\n",
    "- \"confidence\": number in [0,1]\n",
    "\n",
    "Context:\n",
    "{CONTEXT}\n",
    "\n",
    "Question:\n",
    "{task['question']}\n",
    "\n",
    "If something is unknowable from the context, set \"confidence\": 0 and use an empty \"supporting_facts\".\n",
    "\"\"\".strip()\n",
    "    try:\n",
    "        resp = ollama_run(MODEL, prompt)\n",
    "    except Exception as e:\n",
    "        resp = f\"[ERROR running model locally] {e}\"\n",
    "    structured_trials.append(Trial(step=\"structured\", task_id=task[\"id\"], prompt=prompt, response=resp, meta={\"format\":\"json\"}))\n",
    "\n",
    "save_trials(structured_trials, os.path.join(RESULTS_DIR, \"step2_structured.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729054bf",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Step 3 — Few‑shot Prompting\n",
    "\n",
    "- Provide **1–3 short examples** to steer the style & content.\n",
    "- Compare results with Steps 1–2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c855f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"q\": \"List two seasonal drinks in the app and describe them briefly.\",\n",
    "        \"a\": \"- Pumpkin Spice Latte: spiced espresso with milk\\n- Peppermint Mocha: minty chocolate espresso\"\n",
    "    },\n",
    "    {\n",
    "        \"q\": \"Explain what DAU means.\",\n",
    "        \"a\": \"Daily Active Users; the number of unique users opening the app on a given day.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def render_fewshot_block(examples):\n",
    "    parts = [\"You are a helpful assistant. Here are some QA examples:\"]\n",
    "    for ex in examples:\n",
    "        parts.append(f\"Q: {ex['q']}\\nA: {ex['a']}\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "fewshot_prefix = render_fewshot_block(FEW_SHOT_EXAMPLES)\n",
    "\n",
    "fewshot_trials = []\n",
    "for task in TASKS:\n",
    "    prompt = f\"\"\"\n",
    "{fewshot_prefix}\n",
    "\n",
    "Now answer the new question using the context below.\n",
    "\n",
    "Context:\n",
    "{CONTEXT}\n",
    "\n",
    "Question:\n",
    "{task['question']}\n",
    "\"\"\".strip()\n",
    "    try:\n",
    "        resp = ollama_run(MODEL, prompt)\n",
    "    except Exception as e:\n",
    "        resp = f\"[ERROR running model locally] {e}\"\n",
    "    fewshot_trials.append(Trial(step=\"fewshot\", task_id=task[\"id\"], prompt=prompt, response=resp, meta={\"k\": len(FEW_SHOT_EXAMPLES)}))\n",
    "\n",
    "save_trials(fewshot_trials, os.path.join(RESULTS_DIR, \"step3_fewshot.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627a2f",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Step 4 — Chain‑of‑Thought (CoT) Prompting\n",
    "\n",
    "- Encourage explicit reasoning (e.g., _\"Let's think step by step.\"_).  \n",
    "- Compare reasoning quality, correctness, and verbosity with prior steps.\n",
    "\n",
    "> **Note:** When writing your report, avoid pasting raw CoT traces if disallowed by an evaluation policy. Summarize the reasoning instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ade91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cot_trials = []\n",
    "for task in TASKS:\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Let's think step by step and show reasoning before the final answer.\n",
    "Use the context to avoid hallucinations. Provide a short final answer at the end under 'Final Answer: ...'.\n",
    "\n",
    "Context:\n",
    "{CONTEXT}\n",
    "\n",
    "Question:\n",
    "{task['question']}\n",
    "\"\"\".strip()\n",
    "    try:\n",
    "        resp = ollama_run(MODEL, prompt)\n",
    "    except Exception as e:\n",
    "        resp = f\"[ERROR running model locally] {e}\"\n",
    "    cot_trials.append(Trial(step=\"cot\", task_id=task[\"id\"], prompt=prompt, response=resp, meta={\"cot\": True}))\n",
    "\n",
    "save_trials(cot_trials, os.path.join(RESULTS_DIR, \"step4_cot.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbe6c7",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Scoring Scaffold (Manual or Programmatic)\n",
    "\n",
    "Rate each response on:\n",
    "- **Correctness** (0–5), **Completeness** (0–5), **Clarity** (0–5).  \n",
    "Optionally mark factual errors.\n",
    "\n",
    "Fill the table below manually (or write a simple heuristic scorer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9521cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, glob, pandas as pd\n",
    "\n",
    "def load_trials(pattern):\n",
    "    rows = []\n",
    "    for path in sorted(glob.glob(pattern)):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        for item in data:\n",
    "            rows.append({\n",
    "                \"file\": os.path.basename(path),\n",
    "                \"step\": item[\"step\"],\n",
    "                \"task_id\": item[\"task_id\"],\n",
    "                \"prompt\": item[\"prompt\"],\n",
    "                \"response\": item[\"response\"],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_trials(os.path.join(RESULTS_DIR, \"step*.json\"))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add scoring columns and export a CSV for manual rating\n",
    "df_scores = df.copy()\n",
    "for col in [\"correctness_0_5\", \"completeness_0_5\", \"clarity_0_5\", \"notes\"]:\n",
    "    if col not in df_scores.columns:\n",
    "        df_scores[col] = \"\"\n",
    "\n",
    "out_csv = os.path.join(RESULTS_DIR, \"ratings_template.csv\")\n",
    "df_scores.to_csv(out_csv, index=False)\n",
    "out_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e9cb3",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Aggregate & Compare\n",
    "\n",
    "After you fill in `ratings_template.csv`, re‑import and compute per‑step averages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "ratings_path = os.path.join(RESULTS_DIR, \"ratings_template.csv\")\n",
    "ratings = pd.read_csv(ratings_path)\n",
    "\n",
    "def to_num(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "ratings[\"correctness_0_5\"] = ratings[\"correctness_0_5\"].apply(to_num)\n",
    "ratings[\"completeness_0_5\"] = ratings[\"completeness_0_5\"].apply(to_num)\n",
    "ratings[\"clarity_0_5\"] = ratings[\"clarity_0_5\"].apply(to_num)\n",
    "\n",
    "summary = ratings.groupby(\"step\")[[\"correctness_0_5\",\"completeness_0_5\",\"clarity_0_5\"]].mean().round(2)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1a35a",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Visualization\n",
    "\n",
    "Plot average scores by step (Basic vs Structured vs Few‑shot vs CoT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary.plot(kind=\"bar\")\n",
    "plt.title(\"Average Ratings by Prompting Strategy\")\n",
    "plt.ylabel(\"Score (0–5)\")\n",
    "plt.xlabel(\"Strategy\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
