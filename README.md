# MSIT3103-GenAI-assignments


Generated by GPT-5 (high reasoning) via Windsurf 9/4/25
Prompt: "Can you take the data from the directory cifar-10-batches-py, 
ingest it, and load it into sklearn please? I would like to be able to 
train a logistic regression on the dataset"

## CIFAR-10: scikit-learn loader and Logistic Regression

Requirements
- Python 3.9+
- numpy
- scikit-learn

Install deps
```bash
pip install -U numpy scikit-learn
```

Data
- Place/extract the CIFAR-10 python pickles directory at project root as `cifar-10-batches-py/`
- Or pass a custom path via `--data-dir`

Quick start (CLI)
```bash
python train_logreg_cifar10.py --data-dir cifar-10-batches-py \
  --solver saga --max-iter 100 --subset-size 10000
```

Quick start (Python)
```python
from cifar10_loader import load_cifar10
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

train = load_cifar10('cifar-10-batches-py', subset='train', normalize=True, flatten=True)
test = load_cifar10('cifar-10-batches-py', subset='test', normalize=True, flatten=True)

pipe = make_pipeline(
    StandardScaler(),
    LogisticRegression(multi_class='multinomial', solver='saga', max_iter=100)
)
pipe.fit(train.data, train.target)
print('Test accuracy:', pipe.score(test.data, test.target))
```

Notes
- Features are normalized to [0,1] by default; StandardScaler still helps optimization for Logistic Regression.
- Training on full CIFAR-10 (50k x 3,072) with multinomial LR can take time; use `--subset-size` for a quick sanity check.