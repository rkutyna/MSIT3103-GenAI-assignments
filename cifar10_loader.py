# Code generated by GPT-5 (high reasoning) via Windsurf 9/4/25
# Prompt:Can you take the data from the directory cifar-10-batches-py, 
# ingest it, and load it into sklearn please? I would like to be able to 
# train a logistic regression on the dataset

#!/usr/bin/env python3
"""
CIFAR-10 loader for locally downloaded Python pickles (cifar-10-batches-py/).

Returns a scikit-learn compatible `Bunch` with:
- data: np.ndarray, shape (n_samples, 3072) if flatten=True, else (n_samples, 32, 32, 3)
- target: np.ndarray, shape (n_samples,)
- target_names: list[str], class names in order 0..9
- DESCR: str, short description
- images: Optional[np.ndarray], only included when include_images=True or flatten=False

Example
-------
from cifar10_loader import load_cifar10
bunch = load_cifar10("cifar-10-batches-py", subset="train", flatten=True)
X, y = bunch.data, bunch.target
"""
from __future__ import annotations

from pathlib import Path
from typing import List, Tuple, Union, Optional
import pickle
import numpy as np
from sklearn.utils import Bunch

PathLike = Union[str, Path]


def _unpickle(file_path: PathLike) -> dict:
    """Unpickle a CIFAR-10 batch/meta file with Python 3 compatibility."""
    with open(file_path, "rb") as f:
        return pickle.load(f, encoding="latin1")


def _get_label_names(data_dir: PathLike) -> List[str]:
    """Read label names from batches.meta, with a safe fallback."""
    meta_path = Path(data_dir) / "batches.meta"
    default_names = [str(i) for i in range(10)]
    if not meta_path.exists():
        return default_names
    try:
        meta = _unpickle(meta_path)
        names = meta.get("label_names", default_names)
        # Ensure names are strings
        names = [n.decode("utf-8") if isinstance(n, (bytes, bytearray)) else str(n) for n in names]
        return names
    except Exception:
        return default_names


def _extract_xy(batch_dict: dict) -> Tuple[np.ndarray, np.ndarray]:
    """Extract (X, y) from a CIFAR-10 batch dict, handling various key encodings."""
    if "data" in batch_dict:
        X = batch_dict["data"]
    elif b"data" in batch_dict:
        X = batch_dict[b"data"]
    else:
        raise KeyError("CIFAR-10 batch missing 'data' key")

    if "labels" in batch_dict:
        y = batch_dict["labels"]
    elif b"labels" in batch_dict:
        y = batch_dict[b"labels"]
    elif "fine_labels" in batch_dict:
        y = batch_dict["fine_labels"]
    elif b"fine_labels" in batch_dict:
        y = batch_dict[b"fine_labels"]
    else:
        raise KeyError("CIFAR-10 batch missing 'labels' key")

    X = np.asarray(X)
    y = np.asarray(y, dtype=np.int64)
    return X, y


def _to_images(flattened: np.ndarray) -> np.ndarray:
    """Reshape flattened (N, 3072) into (N, 32, 32, 3) in HWC order."""
    images = flattened.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
    return images


def load_cifar10(
    data_dir: PathLike = "cifar-10-batches-py",
    subset: str = "train",
    return_X_y: bool = False,
    normalize: bool = True,
    flatten: bool = True,
    include_images: bool = False,
    dtype: Union[np.dtype, str] = np.float32,
) -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]:
    """
    Load CIFAR-10 from local Python pickles into a scikit-learn Bunch.

    Parameters
    ----------
    data_dir : str or Path
        Path to the 'cifar-10-batches-py' directory.
    subset : {'train', 'test', 'all'}
        Which split to load. 'all' concatenates train and test.
    return_X_y : bool
        If True, return (X, y) instead of a Bunch.
    normalize : bool
        If True, scale pixel values to [0, 1].
    flatten : bool
        If True, return features as shape (N, 3072). If False, return
        X as images of shape (N, 32, 32, 3).
    include_images : bool
        If True, include an 'images' field in the Bunch with shape (N, 32, 32, 3).
        This is always True when flatten is False (since X are images in that case).
    dtype : numpy dtype or str
        Target dtype for returned X.

    Returns
    -------
    Bunch or (X, y)
        If return_X_y=False, a Bunch with fields: data, target, target_names, DESCR,
        and optionally images. Otherwise, a tuple (X, y).
    """
    data_dir = Path(data_dir)
    if not data_dir.exists():
        raise FileNotFoundError(f"CIFAR-10 directory not found: {data_dir}")

    subset = subset.lower()
    if subset not in {"train", "test", "all"}:
        raise ValueError("subset must be one of {'train', 'test', 'all'}")

    batch_files = []
    if subset in {"train", "all"}:
        batch_files += [data_dir / f"data_batch_{i}" for i in range(1, 6)]
    if subset in {"test", "all"}:
        batch_files += [data_dir / "test_batch"]

    X_list: List[np.ndarray] = []
    y_list: List[np.ndarray] = []

    for bf in batch_files:
        if not bf.exists():
            raise FileNotFoundError(f"Expected CIFAR-10 batch file not found: {bf}")
        d = _unpickle(bf)
        Xb, yb = _extract_xy(d)
        X_list.append(np.asarray(Xb))
        y_list.append(np.asarray(yb, dtype=np.int64))

    X = np.vstack(X_list)
    y = np.concatenate(y_list)

    # Prepare images if requested or needed
    images: Optional[np.ndarray] = None
    if not flatten or include_images:
        images = _to_images(X)

    # Prepare final X
    if flatten:
        X_out = X.astype(dtype, copy=False)
        if normalize:
            X_out = X_out / 255.0
    else:
        X_out = images.astype(dtype, copy=False)
        if normalize:
            X_out = X_out / 255.0

    label_names = _get_label_names(data_dir)

    if return_X_y:
        return X_out, y

    descr = (
        "CIFAR-10 dataset loaded from local pickles. "
        f"Split='{subset}', n_samples={X_out.shape[0]}, n_classes={len(label_names)}."
    )

    bunch = Bunch(
        data=X_out,
        target=y,
        target_names=label_names,
        DESCR=descr,
    )
    if images is not None and include_images:
        bunch.images = images

    return bunch


if __name__ == "__main__":
    # Minimal smoke test
    import argparse

    parser = argparse.ArgumentParser(description="Smoke test for CIFAR-10 loader")
    parser.add_argument("--data-dir", type=str, default="cifar-10-batches-py")
    parser.add_argument("--subset", type=str, default="train", choices=["train", "test", "all"])
    parser.add_argument("--no-normalize", action="store_true")
    parser.add_argument("--images", action="store_true", help="Include images in the returned Bunch")
    args = parser.parse_args()

    bunch = load_cifar10(
        data_dir=args.data_dir,
        subset=args.subset,
        normalize=not args.no_normalize,
        flatten=True,
        include_images=args.images,
    )
    X, y = bunch.data, bunch.target
    print("Loaded:", X.shape, y.shape)
    print("Classes:", bunch.target_names)
